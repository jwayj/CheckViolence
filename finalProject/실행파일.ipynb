{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "896d396c-2b72-4fef-9d4a-2a1f731cba12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2080 Ti\n",
      "max_length : 128\n",
      "[train]: 로드 중 파일: ./data/train.csv\n",
      "[train]: 총 1개의 파일에서 365,500개의 데이터를 성공적으로 병합했습니다.\n",
      "[train]: 원본 데이터 365,500개를 10,000개로 샘플링합니다.\n",
      "Train : 10000\n",
      "[valid]: 로드 중 파일: ./data/val.csv\n",
      "[valid]: 총 1개의 파일에서 40,612개의 데이터를 성공적으로 병합했습니다.\n",
      "Valid : 40612\n",
      "[test]: 로드 중 파일: ./data/test.csv\n",
      "[test]: 총 1개의 파일에서 44,998개의 데이터를 성공적으로 병합했습니다.\n",
      "Test : 44998\n",
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2080 Ti\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at snunlp/KR-BERT-char16424 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizer(name_or_path='snunlp/KR-BERT-char16424', vocab_size=16424, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n",
      "The KRBERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (16424, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch    50  of    313.    Elapsed: 0:00:11.\n",
      "  Batch   100  of    313.    Elapsed: 0:00:23.\n",
      "  Batch   150  of    313.    Elapsed: 0:00:34.\n",
      "  Batch   200  of    313.    Elapsed: 0:00:46.\n",
      "  Batch   250  of    313.    Elapsed: 0:00:57.\n",
      "  Batch   300  of    313.    Elapsed: 0:01:09.\n",
      "\n",
      "  Average training loss: 0.51\n",
      "  Training epcoh took: 0:01:12\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.80\n",
      "  Validation Loss: 0.44\n",
      "  Validation took: 0:01:35\n",
      "\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch    50  of    313.    Elapsed: 0:00:12.\n",
      "  Batch   100  of    313.    Elapsed: 0:00:24.\n",
      "  Batch   150  of    313.    Elapsed: 0:00:37.\n",
      "  Batch   200  of    313.    Elapsed: 0:00:49.\n",
      "  Batch   250  of    313.    Elapsed: 0:01:01.\n",
      "  Batch   300  of    313.    Elapsed: 0:01:13.\n",
      "\n",
      "  Average training loss: 0.36\n",
      "  Training epcoh took: 0:01:17\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.80\n",
      "  Validation Loss: 0.45\n",
      "  Validation took: 0:01:38\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:05:41 (h:mm:ss)\n",
      "Predicting labels for 1,407 total test sentences...\n",
      "\n",
      "--- [전체 데이터셋] 테스트 결과 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " 비도덕성 없음 (0)       0.78      0.87      0.82     18987\n",
      " 비도덕성 있음 (1)       0.90      0.82      0.86     26011\n",
      "\n",
      "    accuracy                           0.84     44998\n",
      "   macro avg       0.84      0.85      0.84     44998\n",
      "weighted avg       0.85      0.84      0.84     44998\n",
      "\n",
      "    DONE (Total Test).\n",
      "\n",
      "\n",
      "--- [유형별 상세 테스트 결과] ---\n",
      "\n",
      "--- TEST 데이터를 모든 유형을 기준으로 분리 중... ---\n",
      "  - 유형: CENSURE              | 샘플 수: 19,866\n",
      "  - 유형: ABUSE                | 샘플 수: 1,474\n",
      "  - 유형: HATE                 | 샘플 수: 9,222\n",
      "  - 유형: VIOLENCE             | 샘플 수: 2,167\n",
      "  - 유형: CRIME                | 샘플 수: 1,075\n",
      "  - 유형: SEXUAL               | 샘플 수: 2,737\n",
      "  - 유형: DISCRIMINATION       | 샘플 수: 2,664\n",
      "\n",
      "--- [유형별 테스트 결과] 유형: CENSURE ---\n",
      "총 샘플 수: 19,866\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " 비도덕성 없음 (0)       0.00      0.00      0.00         0\n",
      " 비도덕성 있음 (1)       1.00      0.84      0.91     19866\n",
      "\n",
      "    accuracy                           0.84     19866\n",
      "   macro avg       0.50      0.42      0.46     19866\n",
      "weighted avg       1.00      0.84      0.91     19866\n",
      "\n",
      "\n",
      "--- [유형별 테스트 결과] 유형: ABUSE ---\n",
      "총 샘플 수: 1,474\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " 비도덕성 없음 (0)       0.00      0.00      0.00         0\n",
      " 비도덕성 있음 (1)       1.00      0.88      0.94      1474\n",
      "\n",
      "    accuracy                           0.88      1474\n",
      "   macro avg       0.50      0.44      0.47      1474\n",
      "weighted avg       1.00      0.88      0.94      1474\n",
      "\n",
      "\n",
      "--- [유형별 테스트 결과] 유형: HATE ---\n",
      "총 샘플 수: 9,222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " 비도덕성 없음 (0)       0.00      0.00      0.00         0\n",
      " 비도덕성 있음 (1)       1.00      0.88      0.94      9222\n",
      "\n",
      "    accuracy                           0.88      9222\n",
      "   macro avg       0.50      0.44      0.47      9222\n",
      "weighted avg       1.00      0.88      0.94      9222\n",
      "\n",
      "\n",
      "--- [유형별 테스트 결과] 유형: VIOLENCE ---\n",
      "총 샘플 수: 2,167\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " 비도덕성 없음 (0)       0.00      0.00      0.00         0\n",
      " 비도덕성 있음 (1)       1.00      0.89      0.94      2167\n",
      "\n",
      "    accuracy                           0.89      2167\n",
      "   macro avg       0.50      0.44      0.47      2167\n",
      "weighted avg       1.00      0.89      0.94      2167\n",
      "\n",
      "\n",
      "--- [유형별 테스트 결과] 유형: CRIME ---\n",
      "총 샘플 수: 1,075\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " 비도덕성 없음 (0)       0.00      0.00      0.00         0\n",
      " 비도덕성 있음 (1)       1.00      0.72      0.84      1075\n",
      "\n",
      "    accuracy                           0.72      1075\n",
      "   macro avg       0.50      0.36      0.42      1075\n",
      "weighted avg       1.00      0.72      0.84      1075\n",
      "\n",
      "\n",
      "--- [유형별 테스트 결과] 유형: SEXUAL ---\n",
      "총 샘플 수: 2,737\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " 비도덕성 없음 (0)       0.00      0.00      0.00         0\n",
      " 비도덕성 있음 (1)       1.00      0.76      0.87      2737\n",
      "\n",
      "    accuracy                           0.76      2737\n",
      "   macro avg       0.50      0.38      0.43      2737\n",
      "weighted avg       1.00      0.76      0.87      2737\n",
      "\n",
      "\n",
      "--- [유형별 테스트 결과] 유형: DISCRIMINATION ---\n",
      "총 샘플 수: 2,664\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " 비도덕성 없음 (0)       0.00      0.00      0.00         0\n",
      " 비도덕성 있음 (1)       1.00      0.84      0.91      2664\n",
      "\n",
      "    accuracy                           0.84      2664\n",
      "   macro avg       0.50      0.42      0.46      2664\n",
      "weighted avg       1.00      0.84      0.91      2664\n",
      "\n",
      "\n",
      "--- 유형별 상세 테스트 종료 ---\n",
      "Saving model to ./model_checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -lcufile\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2080 Ti\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at snunlp/KR-BERT-char16424 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./model_checkpoint\n",
      "'이런건 걍 돈없고 멍청한 전라도 여자들 겨냥한 상술이지' 은/는 폭력성이 포함된 문장입니다\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request\n",
    "\n",
    "from dataloader import Dataset\n",
    "from model import Models\n",
    "from transformers import BertTokenizer\n",
    "from utils import split_sentence\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def file_upload():\n",
    "    return render_template('load_file.html')\n",
    "    \n",
    "@app.route('/isViolence', methods = ['POST', 'GET'])\n",
    "def Predict():\n",
    "    if request.method == 'POST':\n",
    "        input_text = request.files['파일'].read().decode('utf-8') # 파일 불러오기\n",
    "        input_text = input_text.split('\\r\\n')\n",
    "        \n",
    "        if input_text == None:\n",
    "            return render_template('isViolence.html', Output = '')\n",
    "\n",
    "        sentences = split_sentence(input_text)  # 채팅 목록을 문장 단위로 분리\n",
    "        # print(sentences)\n",
    "        result = 0\n",
    "        for i, sentence in enumerate(sentences[:10000]):\n",
    "            if (i+1) % 1000 == 0:\n",
    "                print(f\"{i+1}번째 실행 중...\")\n",
    "            result += model.inference(sentence[-1])[1]\n",
    "        \n",
    "        ModelOutput = f\"해당 채팅방의 폭력성 대화 비율은 {round((result / len(sentences)) * 100, 2)}% 입니다\"\n",
    "        print(ModelOutput)\n",
    "        return render_template('isViolence.html', Output = ModelOutput)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    train_file_path = './data/train.csv'\n",
    "    valid_file_path = './data/val.csv'\n",
    "    test_file_path = './data/test.csv' \n",
    "    model_save_dir = './model_checkpoint'       # 모델 저장 디렉토리 경로\n",
    "    model_load_dir = './model_checkpoint'       # 모델 불러오기 디렉토리 경로\n",
    "\n",
    "    TRAIN_DATA_LIMIT = 10000\n",
    "\n",
    "    # # 1. 단일 Dataset 인스턴스 생성 및 데이터 로드\n",
    "    dataset = Dataset()\n",
    "    dataset.set_dataset('train', file_path=train_file_path, max_rows=TRAIN_DATA_LIMIT)\n",
    "    dataset.set_dataset('valid', file_path=valid_file_path)\n",
    "    dataset.set_dataset('test', file_path=test_file_path)\n",
    "\n",
    "    # # 2. Models 인스턴스 생성 및 Dataset 연결\n",
    "    # Models 인스턴스를 만들 때, dataset 객체를 인수로 전달합니다.\n",
    "    model = Models('krbert', num_labels = 2, dataset_instance=dataset) \n",
    "    \n",
    "    # # 3. 모델 정의 및 토크나이저 설정\n",
    "    # model.BERT()를 호출하여 모델을 생성하고, 반환된 토크나이저를 dataset에 설정합니다.\n",
    "    tokenizer_for_training = model.BERT()  \n",
    "    dataset.set_tokenizer(tokenizer_for_training)\n",
    "    \n",
    "    print(dataset.get_tokenizer())\n",
    "    \n",
    "    # # 4. 데이터 로더 생성 (이제 올바른 토크나이저를 사용)\n",
    "    train, valid, test = dataset.get_dataloader()  \n",
    "    \n",
    "    model.about_model()\n",
    "    model.train(train, valid, epochs = 2, project_title=None) \n",
    "    model.test(test)\n",
    "    model.save_model(model_save_dir)\n",
    "\n",
    "    # # 5. 추론 및 웹 서버용 모델 로드 (새로운 인스턴스 생성 및 로드)\n",
    "    # 추론용 모델도 동일한 dataset 인스턴스를 사용하도록 연결\n",
    "    web_model = Models('krbert', num_labels = 2, dataset_instance=dataset)\n",
    "    web_model.load_model(load_dir_path=model_load_dir)\n",
    "    \n",
    "    # # 6. 단일 문장 추론 테스트\n",
    "    sentence, prediction = web_model.inference('이런건 걍 돈없고 멍청한 전라도 여자들 겨냥한 상술이지')\n",
    "    print(f\"'{sentence}' 은/는 폭력성이 포함된 문장입니다\" if prediction == 1 else f\"'{sentence}' 은/는 폭력성이 포함되지 않은 문장입니다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d922900d-cd05-4fad-8a42-2515c97cec0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2080 Ti\n",
      "max_length : 128\n",
      "[train]: 로드 중 파일: ./data/train.csv\n",
      "[train]: 총 1개의 파일에서 365,500개의 데이터를 성공적으로 병합했습니다.\n",
      "[train]: 원본 데이터 365,500개를 50,000개로 샘플링합니다.\n",
      "Train : 50000\n",
      "[valid]: 로드 중 파일: ./data/val.csv\n",
      "[valid]: 총 1개의 파일에서 40,612개의 데이터를 성공적으로 병합했습니다.\n",
      "Valid : 40612\n",
      "[test]: 로드 중 파일: ./data/test.csv\n",
      "[test]: 총 1개의 파일에서 44,998개의 데이터를 성공적으로 병합했습니다.\n",
      "Test : 44998\n",
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2080 Ti\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at snunlp/KR-BERT-char16424 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizer(name_or_path='snunlp/KR-BERT-char16424', vocab_size=16424, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n",
      "The KRBERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (16424, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch    50  of  1,563.    Elapsed: 0:00:11.\n",
      "  Batch   100  of  1,563.    Elapsed: 0:00:22.\n",
      "  Batch   150  of  1,563.    Elapsed: 0:00:34.\n",
      "  Batch   200  of  1,563.    Elapsed: 0:00:45.\n",
      "  Batch   250  of  1,563.    Elapsed: 0:00:57.\n",
      "  Batch   300  of  1,563.    Elapsed: 0:01:09.\n",
      "  Batch   350  of  1,563.    Elapsed: 0:01:20.\n",
      "  Batch   400  of  1,563.    Elapsed: 0:01:32.\n",
      "  Batch   450  of  1,563.    Elapsed: 0:01:44.\n",
      "  Batch   500  of  1,563.    Elapsed: 0:01:56.\n",
      "  Batch   550  of  1,563.    Elapsed: 0:02:08.\n",
      "  Batch   600  of  1,563.    Elapsed: 0:02:20.\n",
      "  Batch   650  of  1,563.    Elapsed: 0:02:32.\n",
      "  Batch   700  of  1,563.    Elapsed: 0:02:45.\n",
      "  Batch   750  of  1,563.    Elapsed: 0:02:57.\n",
      "  Batch   800  of  1,563.    Elapsed: 0:03:09.\n",
      "  Batch   850  of  1,563.    Elapsed: 0:03:21.\n",
      "  Batch   900  of  1,563.    Elapsed: 0:03:33.\n",
      "  Batch   950  of  1,563.    Elapsed: 0:03:45.\n",
      "  Batch 1,000  of  1,563.    Elapsed: 0:03:58.\n",
      "  Batch 1,050  of  1,563.    Elapsed: 0:04:10.\n",
      "  Batch 1,100  of  1,563.    Elapsed: 0:04:22.\n",
      "  Batch 1,150  of  1,563.    Elapsed: 0:04:34.\n",
      "  Batch 1,200  of  1,563.    Elapsed: 0:04:47.\n",
      "  Batch 1,250  of  1,563.    Elapsed: 0:04:59.\n",
      "  Batch 1,300  of  1,563.    Elapsed: 0:05:11.\n",
      "  Batch 1,350  of  1,563.    Elapsed: 0:05:24.\n",
      "  Batch 1,400  of  1,563.    Elapsed: 0:05:36.\n",
      "  Batch 1,450  of  1,563.    Elapsed: 0:05:48.\n",
      "  Batch 1,500  of  1,563.    Elapsed: 0:06:01.\n",
      "  Batch 1,550  of  1,563.    Elapsed: 0:06:13.\n",
      "\n",
      "  Average training loss: 0.43\n",
      "  Training epcoh took: 0:06:16\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.83\n",
      "  Validation Loss: 0.38\n",
      "  Validation took: 0:01:38\n",
      "\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch    50  of  1,563.    Elapsed: 0:00:12.\n",
      "  Batch   100  of  1,563.    Elapsed: 0:00:25.\n",
      "  Batch   150  of  1,563.    Elapsed: 0:00:37.\n",
      "  Batch   200  of  1,563.    Elapsed: 0:00:50.\n",
      "  Batch   250  of  1,563.    Elapsed: 0:01:02.\n",
      "  Batch   300  of  1,563.    Elapsed: 0:01:15.\n",
      "  Batch   350  of  1,563.    Elapsed: 0:01:27.\n",
      "  Batch   400  of  1,563.    Elapsed: 0:01:39.\n",
      "  Batch   450  of  1,563.    Elapsed: 0:01:52.\n",
      "  Batch   500  of  1,563.    Elapsed: 0:02:04.\n",
      "  Batch   550  of  1,563.    Elapsed: 0:02:17.\n",
      "  Batch   600  of  1,563.    Elapsed: 0:02:29.\n",
      "  Batch   650  of  1,563.    Elapsed: 0:02:42.\n",
      "  Batch   700  of  1,563.    Elapsed: 0:02:54.\n",
      "  Batch   750  of  1,563.    Elapsed: 0:03:07.\n",
      "  Batch   800  of  1,563.    Elapsed: 0:03:19.\n",
      "  Batch   850  of  1,563.    Elapsed: 0:03:32.\n",
      "  Batch   900  of  1,563.    Elapsed: 0:03:44.\n",
      "  Batch   950  of  1,563.    Elapsed: 0:03:57.\n",
      "  Batch 1,000  of  1,563.    Elapsed: 0:04:09.\n",
      "  Batch 1,050  of  1,563.    Elapsed: 0:04:21.\n",
      "  Batch 1,100  of  1,563.    Elapsed: 0:04:34.\n",
      "  Batch 1,150  of  1,563.    Elapsed: 0:04:46.\n",
      "  Batch 1,200  of  1,563.    Elapsed: 0:04:59.\n",
      "  Batch 1,250  of  1,563.    Elapsed: 0:05:11.\n",
      "  Batch 1,300  of  1,563.    Elapsed: 0:05:24.\n",
      "  Batch 1,350  of  1,563.    Elapsed: 0:05:36.\n",
      "  Batch 1,400  of  1,563.    Elapsed: 0:05:49.\n",
      "  Batch 1,450  of  1,563.    Elapsed: 0:06:01.\n",
      "  Batch 1,500  of  1,563.    Elapsed: 0:06:14.\n",
      "  Batch 1,550  of  1,563.    Elapsed: 0:06:26.\n",
      "\n",
      "  Average training loss: 0.32\n",
      "  Training epcoh took: 0:06:29\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.83\n",
      "  Validation Loss: 0.41\n",
      "  Validation took: 0:01:38\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:16:01 (h:mm:ss)\n",
      "Predicting labels for 1,407 total test sentences...\n",
      "\n",
      "--- [전체 데이터셋] 테스트 결과 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " 비도덕성 없음 (0)       0.82      0.89      0.85     18987\n",
      " 비도덕성 있음 (1)       0.91      0.86      0.88     26011\n",
      "\n",
      "    accuracy                           0.87     44998\n",
      "   macro avg       0.87      0.87      0.87     44998\n",
      "weighted avg       0.87      0.87      0.87     44998\n",
      "\n",
      "    DONE (Total Test).\n",
      "\n",
      "\n",
      "--- [유형별 상세 테스트 결과] ---\n",
      "\n",
      "--- TEST 데이터를 모든 유형을 기준으로 분리 중... ---\n",
      "  - 유형: CENSURE              | 샘플 수: 19,866\n",
      "  - 유형: ABUSE                | 샘플 수: 1,474\n",
      "  - 유형: HATE                 | 샘플 수: 9,222\n",
      "  - 유형: VIOLENCE             | 샘플 수: 2,167\n",
      "  - 유형: CRIME                | 샘플 수: 1,075\n",
      "  - 유형: SEXUAL               | 샘플 수: 2,737\n",
      "  - 유형: DISCRIMINATION       | 샘플 수: 2,664\n",
      "\n",
      "--- [유형별 테스트 결과] 유형: CENSURE ---\n",
      "총 샘플 수: 19,866\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " 비도덕성 없음 (0)       0.00      0.00      0.00         0\n",
      " 비도덕성 있음 (1)       1.00      0.87      0.93     19866\n",
      "\n",
      "    accuracy                           0.87     19866\n",
      "   macro avg       0.50      0.44      0.47     19866\n",
      "weighted avg       1.00      0.87      0.93     19866\n",
      "\n",
      "\n",
      "--- [유형별 테스트 결과] 유형: ABUSE ---\n",
      "총 샘플 수: 1,474\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " 비도덕성 없음 (0)       0.00      0.00      0.00         0\n",
      " 비도덕성 있음 (1)       1.00      0.91      0.95      1474\n",
      "\n",
      "    accuracy                           0.91      1474\n",
      "   macro avg       0.50      0.46      0.48      1474\n",
      "weighted avg       1.00      0.91      0.95      1474\n",
      "\n",
      "\n",
      "--- [유형별 테스트 결과] 유형: HATE ---\n",
      "총 샘플 수: 9,222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " 비도덕성 없음 (0)       0.00      0.00      0.00         0\n",
      " 비도덕성 있음 (1)       1.00      0.91      0.95      9222\n",
      "\n",
      "    accuracy                           0.91      9222\n",
      "   macro avg       0.50      0.45      0.48      9222\n",
      "weighted avg       1.00      0.91      0.95      9222\n",
      "\n",
      "\n",
      "--- [유형별 테스트 결과] 유형: VIOLENCE ---\n",
      "총 샘플 수: 2,167\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " 비도덕성 없음 (0)       0.00      0.00      0.00         0\n",
      " 비도덕성 있음 (1)       1.00      0.92      0.96      2167\n",
      "\n",
      "    accuracy                           0.92      2167\n",
      "   macro avg       0.50      0.46      0.48      2167\n",
      "weighted avg       1.00      0.92      0.96      2167\n",
      "\n",
      "\n",
      "--- [유형별 테스트 결과] 유형: CRIME ---\n",
      "총 샘플 수: 1,075\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " 비도덕성 없음 (0)       0.00      0.00      0.00         0\n",
      " 비도덕성 있음 (1)       1.00      0.79      0.88      1075\n",
      "\n",
      "    accuracy                           0.79      1075\n",
      "   macro avg       0.50      0.39      0.44      1075\n",
      "weighted avg       1.00      0.79      0.88      1075\n",
      "\n",
      "\n",
      "--- [유형별 테스트 결과] 유형: SEXUAL ---\n",
      "총 샘플 수: 2,737\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " 비도덕성 없음 (0)       0.00      0.00      0.00         0\n",
      " 비도덕성 있음 (1)       1.00      0.80      0.89      2737\n",
      "\n",
      "    accuracy                           0.80      2737\n",
      "   macro avg       0.50      0.40      0.45      2737\n",
      "weighted avg       1.00      0.80      0.89      2737\n",
      "\n",
      "\n",
      "--- [유형별 테스트 결과] 유형: DISCRIMINATION ---\n",
      "총 샘플 수: 2,664\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " 비도덕성 없음 (0)       0.00      0.00      0.00         0\n",
      " 비도덕성 있음 (1)       1.00      0.88      0.93      2664\n",
      "\n",
      "    accuracy                           0.88      2664\n",
      "   macro avg       0.50      0.44      0.47      2664\n",
      "weighted avg       1.00      0.88      0.93      2664\n",
      "\n",
      "\n",
      "--- 유형별 상세 테스트 종료 ---\n",
      "Saving model to ./model_checkpoint\n",
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2080 Ti\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at snunlp/KR-BERT-char16424 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./model_checkpoint\n",
      "'이런건 걍 돈없고 멍청한 전라도 여자들 겨냥한 상술이지' 은/는 폭력성이 포함된 문장입니다\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request\n",
    "\n",
    "from dataloader import Dataset\n",
    "from model import Models\n",
    "from transformers import BertTokenizer\n",
    "from utils import split_sentence\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def file_upload():\n",
    "    return render_template('load_file.html')\n",
    "    \n",
    "@app.route('/isViolence', methods = ['POST', 'GET'])\n",
    "def Predict():\n",
    "    if request.method == 'POST':\n",
    "        input_text = request.files['파일'].read().decode('utf-8') # 파일 불러오기\n",
    "        input_text = input_text.split('\\r\\n')\n",
    "        \n",
    "        if input_text == None:\n",
    "            return render_template('isViolence.html', Output = '')\n",
    "\n",
    "        sentences = split_sentence(input_text)  # 채팅 목록을 문장 단위로 분리\n",
    "        # print(sentences)\n",
    "        result = 0\n",
    "        for i, sentence in enumerate(sentences[:10000]):\n",
    "            if (i+1) % 1000 == 0:\n",
    "                print(f\"{i+1}번째 실행 중...\")\n",
    "            result += model.inference(sentence[-1])[1]\n",
    "        \n",
    "        ModelOutput = f\"해당 채팅방의 폭력성 대화 비율은 {round((result / len(sentences)) * 100, 2)}% 입니다\"\n",
    "        print(ModelOutput)\n",
    "        return render_template('isViolence.html', Output = ModelOutput)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    train_file_path = './data/train.csv'\n",
    "    valid_file_path = './data/val.csv'\n",
    "    test_file_path = './data/test.csv' \n",
    "    model_save_dir = './model_checkpoint'       # 모델 저장 디렉토리 경로\n",
    "    model_load_dir = './model_checkpoint'       # 모델 불러오기 디렉토리 경로\n",
    "\n",
    "    TRAIN_DATA_LIMIT = 50000 \n",
    "\n",
    "    # # 1. 단일 Dataset 인스턴스 생성 및 데이터 로드\n",
    "    dataset = Dataset()\n",
    "    dataset.set_dataset('train', file_path=train_file_path, max_rows=TRAIN_DATA_LIMIT)\n",
    "    dataset.set_dataset('valid', file_path=valid_file_path)\n",
    "    dataset.set_dataset('test', file_path=test_file_path)\n",
    "\n",
    "    # # 2. Models 인스턴스 생성 및 Dataset 연결\n",
    "    # Models 인스턴스를 만들 때, dataset 객체를 인수로 전달합니다.\n",
    "    model = Models('krbert', num_labels = 2, dataset_instance=dataset) \n",
    "    \n",
    "    # # 3. 모델 정의 및 토크나이저 설정\n",
    "    # model.BERT()를 호출하여 모델을 생성하고, 반환된 토크나이저를 dataset에 설정합니다.\n",
    "    tokenizer_for_training = model.BERT()  \n",
    "    dataset.set_tokenizer(tokenizer_for_training)\n",
    "    \n",
    "    print(dataset.get_tokenizer())\n",
    "    \n",
    "    # # 4. 데이터 로더 생성 (이제 올바른 토크나이저를 사용)\n",
    "    train, valid, test = dataset.get_dataloader()  \n",
    "    \n",
    "    model.about_model()\n",
    "    model.train(train, valid, epochs = 2, project_title=None) \n",
    "    model.test(test)\n",
    "    model.save_model(model_save_dir)\n",
    "\n",
    "    # # 5. 추론 및 웹 서버용 모델 로드 (새로운 인스턴스 생성 및 로드)\n",
    "    # 추론용 모델도 동일한 dataset 인스턴스를 사용하도록 연결\n",
    "    web_model = Models('krbert', num_labels = 2, dataset_instance=dataset)\n",
    "    web_model.load_model(load_dir_path=model_load_dir)\n",
    "    \n",
    "    # # 6. 단일 문장 추론 테스트\n",
    "    sentence, prediction = web_model.inference('이런건 걍 돈없고 멍청한 전라도 여자들 겨냥한 상술이지')\n",
    "    print(f\"'{sentence}' 은/는 폭력성이 포함된 문장입니다\" if prediction == 1 else f\"'{sentence}' 은/는 폭력성이 포함되지 않은 문장입니다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5782aba-43aa-4272-8c58-770e8adf1f44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2080 Ti\n",
      "max_length : 128\n",
      "[train]: 로드 중 파일: ./data/train.csv\n",
      "[train]: 총 1개의 파일에서 365,500개의 데이터를 성공적으로 병합했습니다.\n",
      "[train]: 원본 데이터 365,500개를 100,000개로 샘플링합니다.\n",
      "Train : 100000\n",
      "[valid]: 로드 중 파일: ./data/val.csv\n",
      "[valid]: 총 1개의 파일에서 40,612개의 데이터를 성공적으로 병합했습니다.\n",
      "Valid : 40612\n",
      "[test]: 로드 중 파일: ./data/test.csv\n",
      "[test]: 총 1개의 파일에서 44,998개의 데이터를 성공적으로 병합했습니다.\n",
      "Test : 44998\n",
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2080 Ti\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at snunlp/KR-BERT-char16424 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizer(name_or_path='snunlp/KR-BERT-char16424', vocab_size=16424, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n",
      "The KRBERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (16424, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch    50  of  3,125.    Elapsed: 0:00:12.\n",
      "  Batch   100  of  3,125.    Elapsed: 0:00:24.\n",
      "  Batch   150  of  3,125.    Elapsed: 0:00:36.\n",
      "  Batch   200  of  3,125.    Elapsed: 0:00:48.\n",
      "  Batch   250  of  3,125.    Elapsed: 0:01:00.\n",
      "  Batch   300  of  3,125.    Elapsed: 0:01:13.\n",
      "  Batch   350  of  3,125.    Elapsed: 0:01:25.\n",
      "  Batch   400  of  3,125.    Elapsed: 0:01:38.\n",
      "  Batch   450  of  3,125.    Elapsed: 0:01:50.\n",
      "  Batch   500  of  3,125.    Elapsed: 0:02:02.\n",
      "  Batch   550  of  3,125.    Elapsed: 0:02:15.\n",
      "  Batch   600  of  3,125.    Elapsed: 0:02:27.\n",
      "  Batch   650  of  3,125.    Elapsed: 0:02:40.\n",
      "  Batch   700  of  3,125.    Elapsed: 0:02:52.\n",
      "  Batch   750  of  3,125.    Elapsed: 0:03:05.\n",
      "  Batch   800  of  3,125.    Elapsed: 0:03:17.\n",
      "  Batch   850  of  3,125.    Elapsed: 0:03:30.\n",
      "  Batch   900  of  3,125.    Elapsed: 0:03:42.\n",
      "  Batch   950  of  3,125.    Elapsed: 0:03:55.\n",
      "  Batch 1,000  of  3,125.    Elapsed: 0:04:07.\n",
      "  Batch 1,050  of  3,125.    Elapsed: 0:04:20.\n",
      "  Batch 1,100  of  3,125.    Elapsed: 0:04:32.\n",
      "  Batch 1,150  of  3,125.    Elapsed: 0:04:45.\n",
      "  Batch 1,200  of  3,125.    Elapsed: 0:04:57.\n",
      "  Batch 1,250  of  3,125.    Elapsed: 0:05:10.\n",
      "  Batch 1,300  of  3,125.    Elapsed: 0:05:22.\n",
      "  Batch 1,350  of  3,125.    Elapsed: 0:05:35.\n",
      "  Batch 1,400  of  3,125.    Elapsed: 0:05:47.\n",
      "  Batch 1,450  of  3,125.    Elapsed: 0:06:00.\n",
      "  Batch 1,500  of  3,125.    Elapsed: 0:06:12.\n",
      "  Batch 1,550  of  3,125.    Elapsed: 0:06:25.\n",
      "  Batch 1,600  of  3,125.    Elapsed: 0:06:37.\n",
      "  Batch 1,650  of  3,125.    Elapsed: 0:06:50.\n",
      "  Batch 1,700  of  3,125.    Elapsed: 0:07:02.\n",
      "  Batch 1,750  of  3,125.    Elapsed: 0:07:14.\n",
      "  Batch 1,800  of  3,125.    Elapsed: 0:07:27.\n",
      "  Batch 1,850  of  3,125.    Elapsed: 0:07:39.\n",
      "  Batch 1,900  of  3,125.    Elapsed: 0:07:52.\n",
      "  Batch 1,950  of  3,125.    Elapsed: 0:08:04.\n",
      "  Batch 2,000  of  3,125.    Elapsed: 0:08:17.\n",
      "  Batch 2,050  of  3,125.    Elapsed: 0:08:29.\n",
      "  Batch 2,100  of  3,125.    Elapsed: 0:08:42.\n",
      "  Batch 2,150  of  3,125.    Elapsed: 0:08:54.\n",
      "  Batch 2,200  of  3,125.    Elapsed: 0:09:07.\n",
      "  Batch 2,250  of  3,125.    Elapsed: 0:09:19.\n",
      "  Batch 2,300  of  3,125.    Elapsed: 0:09:32.\n",
      "  Batch 2,350  of  3,125.    Elapsed: 0:09:44.\n",
      "  Batch 2,400  of  3,125.    Elapsed: 0:09:56.\n",
      "  Batch 2,450  of  3,125.    Elapsed: 0:10:09.\n",
      "  Batch 2,500  of  3,125.    Elapsed: 0:10:21.\n",
      "  Batch 2,550  of  3,125.    Elapsed: 0:10:34.\n",
      "  Batch 2,600  of  3,125.    Elapsed: 0:10:46.\n",
      "  Batch 2,650  of  3,125.    Elapsed: 0:10:59.\n",
      "  Batch 2,700  of  3,125.    Elapsed: 0:11:11.\n",
      "  Batch 2,750  of  3,125.    Elapsed: 0:11:24.\n",
      "  Batch 2,800  of  3,125.    Elapsed: 0:11:36.\n",
      "  Batch 2,850  of  3,125.    Elapsed: 0:11:49.\n",
      "  Batch 2,900  of  3,125.    Elapsed: 0:12:01.\n",
      "  Batch 2,950  of  3,125.    Elapsed: 0:12:14.\n",
      "  Batch 3,000  of  3,125.    Elapsed: 0:12:26.\n",
      "  Batch 3,050  of  3,125.    Elapsed: 0:12:38.\n",
      "  Batch 3,100  of  3,125.    Elapsed: 0:12:51.\n",
      "\n",
      "  Average training loss: 0.41\n",
      "  Training epcoh took: 0:12:57\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.84\n",
      "  Validation Loss: 0.38\n",
      "  Validation took: 0:01:38\n",
      "\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch    50  of  3,125.    Elapsed: 0:00:12.\n",
      "  Batch   100  of  3,125.    Elapsed: 0:00:25.\n",
      "  Batch   150  of  3,125.    Elapsed: 0:00:37.\n",
      "  Batch   200  of  3,125.    Elapsed: 0:00:50.\n",
      "  Batch   250  of  3,125.    Elapsed: 0:01:02.\n",
      "  Batch   300  of  3,125.    Elapsed: 0:01:15.\n",
      "  Batch   350  of  3,125.    Elapsed: 0:01:27.\n",
      "  Batch   400  of  3,125.    Elapsed: 0:01:39.\n",
      "  Batch   450  of  3,125.    Elapsed: 0:01:52.\n",
      "  Batch   500  of  3,125.    Elapsed: 0:02:04.\n",
      "  Batch   550  of  3,125.    Elapsed: 0:02:17.\n",
      "  Batch   600  of  3,125.    Elapsed: 0:02:29.\n",
      "  Batch   650  of  3,125.    Elapsed: 0:02:42.\n",
      "  Batch   700  of  3,125.    Elapsed: 0:02:54.\n",
      "  Batch   750  of  3,125.    Elapsed: 0:03:07.\n",
      "  Batch   800  of  3,125.    Elapsed: 0:03:19.\n",
      "  Batch   850  of  3,125.    Elapsed: 0:03:32.\n",
      "  Batch   900  of  3,125.    Elapsed: 0:03:44.\n",
      "  Batch   950  of  3,125.    Elapsed: 0:03:56.\n",
      "  Batch 1,000  of  3,125.    Elapsed: 0:04:09.\n",
      "  Batch 1,050  of  3,125.    Elapsed: 0:04:21.\n",
      "  Batch 1,100  of  3,125.    Elapsed: 0:04:34.\n",
      "  Batch 1,150  of  3,125.    Elapsed: 0:04:46.\n",
      "  Batch 1,200  of  3,125.    Elapsed: 0:04:59.\n",
      "  Batch 1,250  of  3,125.    Elapsed: 0:05:11.\n",
      "  Batch 1,300  of  3,125.    Elapsed: 0:05:24.\n",
      "  Batch 1,350  of  3,125.    Elapsed: 0:05:36.\n",
      "  Batch 1,400  of  3,125.    Elapsed: 0:05:49.\n",
      "  Batch 1,450  of  3,125.    Elapsed: 0:06:01.\n",
      "  Batch 1,500  of  3,125.    Elapsed: 0:06:13.\n",
      "  Batch 1,550  of  3,125.    Elapsed: 0:06:26.\n",
      "  Batch 1,600  of  3,125.    Elapsed: 0:06:38.\n",
      "  Batch 1,650  of  3,125.    Elapsed: 0:06:51.\n",
      "  Batch 1,700  of  3,125.    Elapsed: 0:07:03.\n",
      "  Batch 1,750  of  3,125.    Elapsed: 0:07:16.\n",
      "  Batch 1,800  of  3,125.    Elapsed: 0:07:28.\n",
      "  Batch 1,850  of  3,125.    Elapsed: 0:07:41.\n",
      "  Batch 1,900  of  3,125.    Elapsed: 0:07:53.\n",
      "  Batch 1,950  of  3,125.    Elapsed: 0:08:06.\n",
      "  Batch 2,000  of  3,125.    Elapsed: 0:08:18.\n",
      "  Batch 2,050  of  3,125.    Elapsed: 0:08:30.\n",
      "  Batch 2,100  of  3,125.    Elapsed: 0:08:43.\n",
      "  Batch 2,150  of  3,125.    Elapsed: 0:08:55.\n",
      "  Batch 2,200  of  3,125.    Elapsed: 0:09:08.\n",
      "  Batch 2,250  of  3,125.    Elapsed: 0:09:20.\n",
      "  Batch 2,300  of  3,125.    Elapsed: 0:09:33.\n",
      "  Batch 2,350  of  3,125.    Elapsed: 0:09:45.\n",
      "  Batch 2,400  of  3,125.    Elapsed: 0:09:58.\n",
      "  Batch 2,450  of  3,125.    Elapsed: 0:10:10.\n",
      "  Batch 2,500  of  3,125.    Elapsed: 0:10:22.\n",
      "  Batch 2,550  of  3,125.    Elapsed: 0:10:35.\n",
      "  Batch 2,600  of  3,125.    Elapsed: 0:10:47.\n",
      "  Batch 2,650  of  3,125.    Elapsed: 0:11:00.\n",
      "  Batch 2,700  of  3,125.    Elapsed: 0:11:12.\n",
      "  Batch 2,750  of  3,125.    Elapsed: 0:11:25.\n",
      "  Batch 2,800  of  3,125.    Elapsed: 0:11:37.\n",
      "  Batch 2,850  of  3,125.    Elapsed: 0:11:50.\n",
      "  Batch 2,900  of  3,125.    Elapsed: 0:12:02.\n",
      "  Batch 2,950  of  3,125.    Elapsed: 0:12:15.\n",
      "  Batch 3,000  of  3,125.    Elapsed: 0:12:27.\n",
      "  Batch 3,050  of  3,125.    Elapsed: 0:12:39.\n",
      "  Batch 3,100  of  3,125.    Elapsed: 0:12:52.\n",
      "\n",
      "  Average training loss: 0.31\n",
      "  Training epcoh took: 0:12:58\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.84\n",
      "  Validation Loss: 0.39\n",
      "  Validation took: 0:01:38\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:29:12 (h:mm:ss)\n",
      "Predicting labels for 1,407 total test sentences...\n",
      "\n",
      "--- [전체 데이터셋] 테스트 결과 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " 비도덕성 없음 (0)       0.83      0.89      0.86     18987\n",
      " 비도덕성 있음 (1)       0.92      0.87      0.89     26011\n",
      "\n",
      "    accuracy                           0.88     44998\n",
      "   macro avg       0.88      0.88      0.88     44998\n",
      "weighted avg       0.88      0.88      0.88     44998\n",
      "\n",
      "    DONE (Total Test).\n",
      "\n",
      "\n",
      "--- [유형별 상세 테스트 결과] ---\n",
      "\n",
      "--- TEST 데이터를 모든 유형을 기준으로 분리 중... ---\n",
      "  - 유형: CENSURE              | 샘플 수: 19,866\n",
      "  - 유형: ABUSE                | 샘플 수: 1,474\n",
      "  - 유형: HATE                 | 샘플 수: 9,222\n",
      "  - 유형: VIOLENCE             | 샘플 수: 2,167\n",
      "  - 유형: CRIME                | 샘플 수: 1,075\n",
      "  - 유형: SEXUAL               | 샘플 수: 2,737\n",
      "  - 유형: DISCRIMINATION       | 샘플 수: 2,664\n",
      "\n",
      "--- [유형별 테스트 결과] 유형: CENSURE ---\n",
      "총 샘플 수: 19,866\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " 비도덕성 없음 (0)       0.00      0.00      0.00         0\n",
      " 비도덕성 있음 (1)       1.00      0.88      0.94     19866\n",
      "\n",
      "    accuracy                           0.88     19866\n",
      "   macro avg       0.50      0.44      0.47     19866\n",
      "weighted avg       1.00      0.88      0.94     19866\n",
      "\n",
      "\n",
      "--- [유형별 테스트 결과] 유형: ABUSE ---\n",
      "총 샘플 수: 1,474\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " 비도덕성 없음 (0)       0.00      0.00      0.00         0\n",
      " 비도덕성 있음 (1)       1.00      0.92      0.96      1474\n",
      "\n",
      "    accuracy                           0.92      1474\n",
      "   macro avg       0.50      0.46      0.48      1474\n",
      "weighted avg       1.00      0.92      0.96      1474\n",
      "\n",
      "\n",
      "--- [유형별 테스트 결과] 유형: HATE ---\n",
      "총 샘플 수: 9,222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " 비도덕성 없음 (0)       0.00      0.00      0.00         0\n",
      " 비도덕성 있음 (1)       1.00      0.92      0.96      9222\n",
      "\n",
      "    accuracy                           0.92      9222\n",
      "   macro avg       0.50      0.46      0.48      9222\n",
      "weighted avg       1.00      0.92      0.96      9222\n",
      "\n",
      "\n",
      "--- [유형별 테스트 결과] 유형: VIOLENCE ---\n",
      "총 샘플 수: 2,167\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " 비도덕성 없음 (0)       0.00      0.00      0.00         0\n",
      " 비도덕성 있음 (1)       1.00      0.93      0.96      2167\n",
      "\n",
      "    accuracy                           0.93      2167\n",
      "   macro avg       0.50      0.46      0.48      2167\n",
      "weighted avg       1.00      0.93      0.96      2167\n",
      "\n",
      "\n",
      "--- [유형별 테스트 결과] 유형: CRIME ---\n",
      "총 샘플 수: 1,075\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " 비도덕성 없음 (0)       0.00      0.00      0.00         0\n",
      " 비도덕성 있음 (1)       1.00      0.79      0.88      1075\n",
      "\n",
      "    accuracy                           0.79      1075\n",
      "   macro avg       0.50      0.40      0.44      1075\n",
      "weighted avg       1.00      0.79      0.88      1075\n",
      "\n",
      "\n",
      "--- [유형별 테스트 결과] 유형: SEXUAL ---\n",
      "총 샘플 수: 2,737\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " 비도덕성 없음 (0)       0.00      0.00      0.00         0\n",
      " 비도덕성 있음 (1)       1.00      0.81      0.90      2737\n",
      "\n",
      "    accuracy                           0.81      2737\n",
      "   macro avg       0.50      0.41      0.45      2737\n",
      "weighted avg       1.00      0.81      0.90      2737\n",
      "\n",
      "\n",
      "--- [유형별 테스트 결과] 유형: DISCRIMINATION ---\n",
      "총 샘플 수: 2,664\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " 비도덕성 없음 (0)       0.00      0.00      0.00         0\n",
      " 비도덕성 있음 (1)       1.00      0.90      0.94      2664\n",
      "\n",
      "    accuracy                           0.90      2664\n",
      "   macro avg       0.50      0.45      0.47      2664\n",
      "weighted avg       1.00      0.90      0.94      2664\n",
      "\n",
      "\n",
      "--- 유형별 상세 테스트 종료 ---\n",
      "Saving model to ./model_checkpoint\n",
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2080 Ti\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at snunlp/KR-BERT-char16424 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./model_checkpoint\n",
      "'이런건 걍 돈없고 멍청한 전라도 여자들 겨냥한 상술이지' 은/는 폭력성이 포함된 문장입니다\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request\n",
    "\n",
    "from dataloader import Dataset\n",
    "from model import Models\n",
    "from transformers import BertTokenizer\n",
    "from utils import split_sentence\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def file_upload():\n",
    "    return render_template('load_file.html')\n",
    "    \n",
    "@app.route('/isViolence', methods = ['POST', 'GET'])\n",
    "def Predict():\n",
    "    if request.method == 'POST':\n",
    "        input_text = request.files['파일'].read().decode('utf-8') # 파일 불러오기\n",
    "        input_text = input_text.split('\\r\\n')\n",
    "        \n",
    "        if input_text == None:\n",
    "            return render_template('isViolence.html', Output = '')\n",
    "\n",
    "        sentences = split_sentence(input_text)  # 채팅 목록을 문장 단위로 분리\n",
    "        # print(sentences)\n",
    "        result = 0\n",
    "        for i, sentence in enumerate(sentences[:10000]):\n",
    "            if (i+1) % 1000 == 0:\n",
    "                print(f\"{i+1}번째 실행 중...\")\n",
    "            result += model.inference(sentence[-1])[1]\n",
    "        \n",
    "        ModelOutput = f\"해당 채팅방의 폭력성 대화 비율은 {round((result / len(sentences)) * 100, 2)}% 입니다\"\n",
    "        print(ModelOutput)\n",
    "        return render_template('isViolence.html', Output = ModelOutput)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    train_file_path = './data/train.csv'\n",
    "    valid_file_path = './data/val.csv'\n",
    "    test_file_path = './data/test.csv' \n",
    "    model_save_dir = './model_checkpoint'       # 모델 저장 디렉토리 경로\n",
    "    model_load_dir = './model_checkpoint'       # 모델 불러오기 디렉토리 경로\n",
    "\n",
    "    TRAIN_DATA_LIMIT = 100000\n",
    "\n",
    "    # # 1. 단일 Dataset 인스턴스 생성 및 데이터 로드\n",
    "    dataset = Dataset()\n",
    "    dataset.set_dataset('train', file_path=train_file_path, max_rows=TRAIN_DATA_LIMIT)\n",
    "    dataset.set_dataset('valid', file_path=valid_file_path)\n",
    "    dataset.set_dataset('test', file_path=test_file_path)\n",
    "\n",
    "    # # 2. Models 인스턴스 생성 및 Dataset 연결\n",
    "    # Models 인스턴스를 만들 때, dataset 객체를 인수로 전달합니다.\n",
    "    model = Models('krbert', num_labels = 2, dataset_instance=dataset) \n",
    "    \n",
    "    # # 3. 모델 정의 및 토크나이저 설정\n",
    "    # model.BERT()를 호출하여 모델을 생성하고, 반환된 토크나이저를 dataset에 설정합니다.\n",
    "    tokenizer_for_training = model.BERT()  \n",
    "    dataset.set_tokenizer(tokenizer_for_training)\n",
    "    \n",
    "    print(dataset.get_tokenizer())\n",
    "    \n",
    "    # # 4. 데이터 로더 생성 (이제 올바른 토크나이저를 사용)\n",
    "    train, valid, test = dataset.get_dataloader()  \n",
    "    \n",
    "    model.about_model()\n",
    "    model.train(train, valid, epochs = 2, project_title=None) \n",
    "    model.test(test)\n",
    "    model.save_model(model_save_dir)\n",
    "\n",
    "    # # 5. 추론 및 웹 서버용 모델 로드 (새로운 인스턴스 생성 및 로드)\n",
    "    # 추론용 모델도 동일한 dataset 인스턴스를 사용하도록 연결\n",
    "    web_model = Models('krbert', num_labels = 2, dataset_instance=dataset)\n",
    "    web_model.load_model(load_dir_path=model_load_dir)\n",
    "    \n",
    "    # # 6. 단일 문장 추론 테스트\n",
    "    sentence, prediction = web_model.inference('이런건 걍 돈없고 멍청한 전라도 여자들 겨냥한 상술이지')\n",
    "    print(f\"'{sentence}' 은/는 폭력성이 포함된 문장입니다\" if prediction == 1 else f\"'{sentence}' 은/는 폭력성이 포함되지 않은 문장입니다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b68474b2-425e-4786-8907-40ab3ab08470",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2080 Ti\n",
      "max_length : 128\n",
      "[train]: 로드 중 파일: ./data/train.csv\n",
      "[train]: 총 1개의 파일에서 365,500개의 데이터를 성공적으로 병합했습니다.\n",
      "[train]: 원본 데이터 365,500개를 200,000개로 샘플링합니다.\n",
      "Train : 200000\n",
      "[valid]: 로드 중 파일: ./data/val.csv\n",
      "[valid]: 총 1개의 파일에서 40,612개의 데이터를 성공적으로 병합했습니다.\n",
      "Valid : 40612\n",
      "[test]: 로드 중 파일: ./data/test.csv\n",
      "[test]: 총 1개의 파일에서 44,998개의 데이터를 성공적으로 병합했습니다.\n",
      "Test : 44998\n",
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2080 Ti\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at snunlp/KR-BERT-char16424 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizer(name_or_path='snunlp/KR-BERT-char16424', vocab_size=16424, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n",
      "The KRBERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (16424, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch    50  of  6,250.    Elapsed: 0:00:12.\n",
      "  Batch   100  of  6,250.    Elapsed: 0:00:23.\n",
      "  Batch   150  of  6,250.    Elapsed: 0:00:35.\n",
      "  Batch   200  of  6,250.    Elapsed: 0:00:48.\n",
      "  Batch   250  of  6,250.    Elapsed: 0:01:00.\n",
      "  Batch   300  of  6,250.    Elapsed: 0:01:12.\n",
      "  Batch   350  of  6,250.    Elapsed: 0:01:24.\n",
      "  Batch   400  of  6,250.    Elapsed: 0:01:37.\n",
      "  Batch   450  of  6,250.    Elapsed: 0:01:49.\n",
      "  Batch   500  of  6,250.    Elapsed: 0:02:01.\n",
      "  Batch   550  of  6,250.    Elapsed: 0:02:14.\n",
      "  Batch   600  of  6,250.    Elapsed: 0:02:26.\n",
      "  Batch   650  of  6,250.    Elapsed: 0:02:39.\n",
      "  Batch   700  of  6,250.    Elapsed: 0:02:51.\n",
      "  Batch   750  of  6,250.    Elapsed: 0:03:04.\n",
      "  Batch   800  of  6,250.    Elapsed: 0:03:16.\n",
      "  Batch   850  of  6,250.    Elapsed: 0:03:28.\n",
      "  Batch   900  of  6,250.    Elapsed: 0:03:41.\n",
      "  Batch   950  of  6,250.    Elapsed: 0:03:53.\n",
      "  Batch 1,000  of  6,250.    Elapsed: 0:04:06.\n",
      "  Batch 1,050  of  6,250.    Elapsed: 0:04:18.\n",
      "  Batch 1,100  of  6,250.    Elapsed: 0:04:31.\n",
      "  Batch 1,150  of  6,250.    Elapsed: 0:04:43.\n",
      "  Batch 1,200  of  6,250.    Elapsed: 0:04:56.\n",
      "  Batch 1,250  of  6,250.    Elapsed: 0:05:08.\n",
      "  Batch 1,300  of  6,250.    Elapsed: 0:05:20.\n",
      "  Batch 1,350  of  6,250.    Elapsed: 0:05:33.\n",
      "  Batch 1,400  of  6,250.    Elapsed: 0:05:45.\n",
      "  Batch 1,450  of  6,250.    Elapsed: 0:05:58.\n",
      "  Batch 1,500  of  6,250.    Elapsed: 0:06:10.\n",
      "  Batch 1,550  of  6,250.    Elapsed: 0:06:23.\n",
      "  Batch 1,600  of  6,250.    Elapsed: 0:06:35.\n",
      "  Batch 1,650  of  6,250.    Elapsed: 0:06:48.\n",
      "  Batch 1,700  of  6,250.    Elapsed: 0:07:00.\n",
      "  Batch 1,750  of  6,250.    Elapsed: 0:07:13.\n",
      "  Batch 1,800  of  6,250.    Elapsed: 0:07:25.\n",
      "  Batch 1,850  of  6,250.    Elapsed: 0:07:37.\n",
      "  Batch 1,900  of  6,250.    Elapsed: 0:07:50.\n",
      "  Batch 1,950  of  6,250.    Elapsed: 0:08:02.\n",
      "  Batch 2,000  of  6,250.    Elapsed: 0:08:15.\n",
      "  Batch 2,050  of  6,250.    Elapsed: 0:08:27.\n",
      "  Batch 2,100  of  6,250.    Elapsed: 0:08:40.\n",
      "  Batch 2,150  of  6,250.    Elapsed: 0:08:52.\n",
      "  Batch 2,200  of  6,250.    Elapsed: 0:09:04.\n",
      "  Batch 2,250  of  6,250.    Elapsed: 0:09:17.\n",
      "  Batch 2,300  of  6,250.    Elapsed: 0:09:29.\n",
      "  Batch 2,350  of  6,250.    Elapsed: 0:09:42.\n",
      "  Batch 2,400  of  6,250.    Elapsed: 0:09:54.\n",
      "  Batch 2,450  of  6,250.    Elapsed: 0:10:07.\n",
      "  Batch 2,500  of  6,250.    Elapsed: 0:10:19.\n",
      "  Batch 2,550  of  6,250.    Elapsed: 0:10:32.\n",
      "  Batch 2,600  of  6,250.    Elapsed: 0:10:44.\n",
      "  Batch 2,650  of  6,250.    Elapsed: 0:10:56.\n",
      "  Batch 2,700  of  6,250.    Elapsed: 0:11:09.\n",
      "  Batch 2,750  of  6,250.    Elapsed: 0:11:21.\n",
      "  Batch 2,800  of  6,250.    Elapsed: 0:11:34.\n",
      "  Batch 2,850  of  6,250.    Elapsed: 0:11:46.\n",
      "  Batch 2,900  of  6,250.    Elapsed: 0:11:59.\n",
      "  Batch 2,950  of  6,250.    Elapsed: 0:12:11.\n",
      "  Batch 3,000  of  6,250.    Elapsed: 0:12:23.\n",
      "  Batch 3,050  of  6,250.    Elapsed: 0:12:36.\n",
      "  Batch 3,100  of  6,250.    Elapsed: 0:12:48.\n",
      "  Batch 3,150  of  6,250.    Elapsed: 0:13:01.\n",
      "  Batch 3,200  of  6,250.    Elapsed: 0:13:13.\n",
      "  Batch 3,250  of  6,250.    Elapsed: 0:13:26.\n",
      "  Batch 3,300  of  6,250.    Elapsed: 0:13:38.\n",
      "  Batch 3,350  of  6,250.    Elapsed: 0:13:51.\n",
      "  Batch 3,400  of  6,250.    Elapsed: 0:14:03.\n",
      "  Batch 3,450  of  6,250.    Elapsed: 0:14:15.\n",
      "  Batch 3,500  of  6,250.    Elapsed: 0:14:28.\n",
      "  Batch 3,550  of  6,250.    Elapsed: 0:14:40.\n",
      "  Batch 3,600  of  6,250.    Elapsed: 0:14:53.\n",
      "  Batch 3,650  of  6,250.    Elapsed: 0:15:05.\n",
      "  Batch 3,700  of  6,250.    Elapsed: 0:15:18.\n",
      "  Batch 3,750  of  6,250.    Elapsed: 0:15:30.\n",
      "  Batch 3,800  of  6,250.    Elapsed: 0:15:43.\n",
      "  Batch 3,850  of  6,250.    Elapsed: 0:15:55.\n",
      "  Batch 3,900  of  6,250.    Elapsed: 0:16:07.\n",
      "  Batch 3,950  of  6,250.    Elapsed: 0:16:20.\n",
      "  Batch 4,000  of  6,250.    Elapsed: 0:16:32.\n",
      "  Batch 4,050  of  6,250.    Elapsed: 0:16:45.\n",
      "  Batch 4,100  of  6,250.    Elapsed: 0:16:57.\n",
      "  Batch 4,150  of  6,250.    Elapsed: 0:17:10.\n",
      "  Batch 4,200  of  6,250.    Elapsed: 0:17:22.\n",
      "  Batch 4,250  of  6,250.    Elapsed: 0:17:35.\n",
      "  Batch 4,300  of  6,250.    Elapsed: 0:17:47.\n",
      "  Batch 4,350  of  6,250.    Elapsed: 0:18:00.\n",
      "  Batch 4,400  of  6,250.    Elapsed: 0:18:12.\n",
      "  Batch 4,450  of  6,250.    Elapsed: 0:18:24.\n",
      "  Batch 4,500  of  6,250.    Elapsed: 0:18:37.\n",
      "  Batch 4,550  of  6,250.    Elapsed: 0:18:49.\n",
      "  Batch 4,600  of  6,250.    Elapsed: 0:19:02.\n",
      "  Batch 4,650  of  6,250.    Elapsed: 0:19:14.\n",
      "  Batch 4,700  of  6,250.    Elapsed: 0:19:27.\n",
      "  Batch 4,750  of  6,250.    Elapsed: 0:19:39.\n",
      "  Batch 4,800  of  6,250.    Elapsed: 0:19:52.\n",
      "  Batch 4,850  of  6,250.    Elapsed: 0:20:04.\n",
      "  Batch 4,900  of  6,250.    Elapsed: 0:20:16.\n",
      "  Batch 4,950  of  6,250.    Elapsed: 0:20:29.\n",
      "  Batch 5,000  of  6,250.    Elapsed: 0:20:41.\n",
      "  Batch 5,050  of  6,250.    Elapsed: 0:20:54.\n",
      "  Batch 5,100  of  6,250.    Elapsed: 0:21:06.\n",
      "  Batch 5,150  of  6,250.    Elapsed: 0:21:19.\n",
      "  Batch 5,200  of  6,250.    Elapsed: 0:21:31.\n",
      "  Batch 5,250  of  6,250.    Elapsed: 0:21:44.\n",
      "  Batch 5,300  of  6,250.    Elapsed: 0:21:56.\n",
      "  Batch 5,350  of  6,250.    Elapsed: 0:22:08.\n",
      "  Batch 5,400  of  6,250.    Elapsed: 0:22:21.\n",
      "  Batch 5,450  of  6,250.    Elapsed: 0:22:33.\n",
      "  Batch 5,500  of  6,250.    Elapsed: 0:22:46.\n",
      "  Batch 5,550  of  6,250.    Elapsed: 0:22:58.\n",
      "  Batch 5,600  of  6,250.    Elapsed: 0:23:11.\n",
      "  Batch 5,650  of  6,250.    Elapsed: 0:23:23.\n",
      "  Batch 5,700  of  6,250.    Elapsed: 0:23:36.\n",
      "  Batch 5,750  of  6,250.    Elapsed: 0:23:48.\n",
      "  Batch 5,800  of  6,250.    Elapsed: 0:24:01.\n",
      "  Batch 5,850  of  6,250.    Elapsed: 0:24:13.\n",
      "  Batch 5,900  of  6,250.    Elapsed: 0:24:25.\n",
      "  Batch 5,950  of  6,250.    Elapsed: 0:24:38.\n",
      "  Batch 6,000  of  6,250.    Elapsed: 0:24:50.\n",
      "  Batch 6,050  of  6,250.    Elapsed: 0:25:03.\n",
      "  Batch 6,100  of  6,250.    Elapsed: 0:25:15.\n",
      "  Batch 6,150  of  6,250.    Elapsed: 0:25:28.\n",
      "  Batch 6,200  of  6,250.    Elapsed: 0:25:40.\n",
      "\n",
      "  Average training loss: 0.39\n",
      "  Training epcoh took: 0:25:53\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.85\n",
      "  Validation Loss: 0.36\n",
      "  Validation took: 0:01:38\n",
      "\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch    50  of  6,250.    Elapsed: 0:00:12.\n",
      "  Batch   100  of  6,250.    Elapsed: 0:00:25.\n",
      "  Batch   150  of  6,250.    Elapsed: 0:00:37.\n",
      "  Batch   200  of  6,250.    Elapsed: 0:00:50.\n",
      "  Batch   250  of  6,250.    Elapsed: 0:01:02.\n",
      "  Batch   300  of  6,250.    Elapsed: 0:01:15.\n",
      "  Batch   350  of  6,250.    Elapsed: 0:01:27.\n",
      "  Batch   400  of  6,250.    Elapsed: 0:01:39.\n",
      "  Batch   450  of  6,250.    Elapsed: 0:01:52.\n",
      "  Batch   500  of  6,250.    Elapsed: 0:02:04.\n",
      "  Batch   550  of  6,250.    Elapsed: 0:02:17.\n",
      "  Batch   600  of  6,250.    Elapsed: 0:02:29.\n",
      "  Batch   650  of  6,250.    Elapsed: 0:02:42.\n",
      "  Batch   700  of  6,250.    Elapsed: 0:02:54.\n",
      "  Batch   750  of  6,250.    Elapsed: 0:03:07.\n",
      "  Batch   800  of  6,250.    Elapsed: 0:03:19.\n",
      "  Batch   850  of  6,250.    Elapsed: 0:03:31.\n",
      "  Batch   900  of  6,250.    Elapsed: 0:03:44.\n",
      "  Batch   950  of  6,250.    Elapsed: 0:03:56.\n",
      "  Batch 1,000  of  6,250.    Elapsed: 0:04:09.\n",
      "  Batch 1,050  of  6,250.    Elapsed: 0:04:21.\n",
      "  Batch 1,100  of  6,250.    Elapsed: 0:04:34.\n",
      "  Batch 1,150  of  6,250.    Elapsed: 0:04:46.\n",
      "  Batch 1,200  of  6,250.    Elapsed: 0:04:59.\n",
      "  Batch 1,250  of  6,250.    Elapsed: 0:05:11.\n",
      "  Batch 1,300  of  6,250.    Elapsed: 0:05:23.\n",
      "  Batch 1,350  of  6,250.    Elapsed: 0:05:36.\n",
      "  Batch 1,400  of  6,250.    Elapsed: 0:05:48.\n",
      "  Batch 1,450  of  6,250.    Elapsed: 0:06:01.\n",
      "  Batch 1,500  of  6,250.    Elapsed: 0:06:13.\n",
      "  Batch 1,550  of  6,250.    Elapsed: 0:06:26.\n",
      "  Batch 1,600  of  6,250.    Elapsed: 0:06:38.\n",
      "  Batch 1,650  of  6,250.    Elapsed: 0:06:51.\n",
      "  Batch 1,700  of  6,250.    Elapsed: 0:07:03.\n",
      "  Batch 1,750  of  6,250.    Elapsed: 0:07:15.\n",
      "  Batch 1,800  of  6,250.    Elapsed: 0:07:28.\n",
      "  Batch 1,850  of  6,250.    Elapsed: 0:07:40.\n",
      "  Batch 1,900  of  6,250.    Elapsed: 0:07:53.\n",
      "  Batch 1,950  of  6,250.    Elapsed: 0:08:05.\n",
      "  Batch 2,000  of  6,250.    Elapsed: 0:08:18.\n",
      "  Batch 2,050  of  6,250.    Elapsed: 0:08:30.\n",
      "  Batch 2,100  of  6,250.    Elapsed: 0:08:43.\n",
      "  Batch 2,150  of  6,250.    Elapsed: 0:08:55.\n",
      "  Batch 2,200  of  6,250.    Elapsed: 0:09:07.\n",
      "  Batch 2,250  of  6,250.    Elapsed: 0:09:20.\n",
      "  Batch 2,300  of  6,250.    Elapsed: 0:09:32.\n",
      "  Batch 2,350  of  6,250.    Elapsed: 0:09:45.\n",
      "  Batch 2,400  of  6,250.    Elapsed: 0:09:57.\n",
      "  Batch 2,450  of  6,250.    Elapsed: 0:10:10.\n",
      "  Batch 2,500  of  6,250.    Elapsed: 0:10:22.\n",
      "  Batch 2,550  of  6,250.    Elapsed: 0:10:34.\n",
      "  Batch 2,600  of  6,250.    Elapsed: 0:10:47.\n",
      "  Batch 2,650  of  6,250.    Elapsed: 0:10:59.\n",
      "  Batch 2,700  of  6,250.    Elapsed: 0:11:12.\n",
      "  Batch 2,750  of  6,250.    Elapsed: 0:11:24.\n",
      "  Batch 2,800  of  6,250.    Elapsed: 0:11:37.\n",
      "  Batch 2,850  of  6,250.    Elapsed: 0:11:49.\n",
      "  Batch 2,900  of  6,250.    Elapsed: 0:12:01.\n",
      "  Batch 2,950  of  6,250.    Elapsed: 0:12:14.\n",
      "  Batch 3,000  of  6,250.    Elapsed: 0:12:26.\n",
      "  Batch 3,050  of  6,250.    Elapsed: 0:12:39.\n",
      "  Batch 3,100  of  6,250.    Elapsed: 0:12:51.\n",
      "  Batch 3,150  of  6,250.    Elapsed: 0:13:04.\n",
      "  Batch 3,200  of  6,250.    Elapsed: 0:13:16.\n",
      "  Batch 3,250  of  6,250.    Elapsed: 0:13:29.\n",
      "  Batch 3,300  of  6,250.    Elapsed: 0:13:41.\n",
      "  Batch 3,350  of  6,250.    Elapsed: 0:13:53.\n",
      "  Batch 3,400  of  6,250.    Elapsed: 0:14:06.\n",
      "  Batch 3,450  of  6,250.    Elapsed: 0:14:18.\n",
      "  Batch 3,500  of  6,250.    Elapsed: 0:14:31.\n",
      "  Batch 3,550  of  6,250.    Elapsed: 0:14:43.\n",
      "  Batch 3,600  of  6,250.    Elapsed: 0:14:56.\n",
      "  Batch 3,650  of  6,250.    Elapsed: 0:15:08.\n",
      "  Batch 3,700  of  6,250.    Elapsed: 0:15:20.\n",
      "  Batch 3,750  of  6,250.    Elapsed: 0:15:33.\n",
      "  Batch 3,800  of  6,250.    Elapsed: 0:15:45.\n",
      "  Batch 3,850  of  6,250.    Elapsed: 0:15:58.\n",
      "  Batch 3,900  of  6,250.    Elapsed: 0:16:10.\n",
      "  Batch 3,950  of  6,250.    Elapsed: 0:16:22.\n",
      "  Batch 4,000  of  6,250.    Elapsed: 0:16:35.\n",
      "  Batch 4,050  of  6,250.    Elapsed: 0:16:47.\n",
      "  Batch 4,100  of  6,250.    Elapsed: 0:17:00.\n",
      "  Batch 4,150  of  6,250.    Elapsed: 0:17:12.\n",
      "  Batch 4,200  of  6,250.    Elapsed: 0:17:24.\n",
      "  Batch 4,250  of  6,250.    Elapsed: 0:17:37.\n",
      "  Batch 4,300  of  6,250.    Elapsed: 0:17:49.\n",
      "  Batch 4,350  of  6,250.    Elapsed: 0:18:02.\n",
      "  Batch 4,400  of  6,250.    Elapsed: 0:18:14.\n",
      "  Batch 4,450  of  6,250.    Elapsed: 0:18:26.\n",
      "  Batch 4,500  of  6,250.    Elapsed: 0:18:39.\n",
      "  Batch 4,550  of  6,250.    Elapsed: 0:18:51.\n",
      "  Batch 4,600  of  6,250.    Elapsed: 0:19:04.\n",
      "  Batch 4,650  of  6,250.    Elapsed: 0:19:16.\n",
      "  Batch 4,700  of  6,250.    Elapsed: 0:19:28.\n",
      "  Batch 4,750  of  6,250.    Elapsed: 0:19:41.\n",
      "  Batch 4,800  of  6,250.    Elapsed: 0:19:53.\n",
      "  Batch 4,850  of  6,250.    Elapsed: 0:20:06.\n",
      "  Batch 4,900  of  6,250.    Elapsed: 0:20:18.\n",
      "  Batch 4,950  of  6,250.    Elapsed: 0:20:31.\n",
      "  Batch 5,000  of  6,250.    Elapsed: 0:20:43.\n",
      "  Batch 5,050  of  6,250.    Elapsed: 0:20:55.\n",
      "  Batch 5,100  of  6,250.    Elapsed: 0:21:08.\n",
      "  Batch 5,150  of  6,250.    Elapsed: 0:21:20.\n",
      "  Batch 5,200  of  6,250.    Elapsed: 0:21:33.\n",
      "  Batch 5,250  of  6,250.    Elapsed: 0:21:45.\n",
      "  Batch 5,300  of  6,250.    Elapsed: 0:21:57.\n",
      "  Batch 5,350  of  6,250.    Elapsed: 0:22:10.\n",
      "  Batch 5,400  of  6,250.    Elapsed: 0:22:22.\n",
      "  Batch 5,450  of  6,250.    Elapsed: 0:22:35.\n",
      "  Batch 5,500  of  6,250.    Elapsed: 0:22:47.\n",
      "  Batch 5,550  of  6,250.    Elapsed: 0:23:00.\n",
      "  Batch 5,600  of  6,250.    Elapsed: 0:23:12.\n",
      "  Batch 5,650  of  6,250.    Elapsed: 0:23:24.\n",
      "  Batch 5,700  of  6,250.    Elapsed: 0:23:37.\n",
      "  Batch 5,750  of  6,250.    Elapsed: 0:23:49.\n",
      "  Batch 5,800  of  6,250.    Elapsed: 0:24:02.\n",
      "  Batch 5,850  of  6,250.    Elapsed: 0:24:14.\n",
      "  Batch 5,900  of  6,250.    Elapsed: 0:24:27.\n",
      "  Batch 5,950  of  6,250.    Elapsed: 0:24:39.\n",
      "  Batch 6,000  of  6,250.    Elapsed: 0:24:52.\n",
      "  Batch 6,050  of  6,250.    Elapsed: 0:25:04.\n",
      "  Batch 6,100  of  6,250.    Elapsed: 0:25:16.\n",
      "  Batch 6,150  of  6,250.    Elapsed: 0:25:29.\n",
      "  Batch 6,200  of  6,250.    Elapsed: 0:25:41.\n",
      "\n",
      "  Average training loss: 0.30\n",
      "  Training epcoh took: 0:25:54\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.85\n",
      "  Validation Loss: 0.37\n",
      "  Validation took: 0:01:38\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:55:03 (h:mm:ss)\n",
      "Predicting labels for 1,407 total test sentences...\n",
      "\n",
      "--- [전체 데이터셋] 테스트 결과 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " 비도덕성 없음 (0)       0.85      0.90      0.87     18987\n",
      " 비도덕성 있음 (1)       0.92      0.88      0.90     26011\n",
      "\n",
      "    accuracy                           0.89     44998\n",
      "   macro avg       0.89      0.89      0.89     44998\n",
      "weighted avg       0.89      0.89      0.89     44998\n",
      "\n",
      "    DONE (Total Test).\n",
      "\n",
      "\n",
      "--- [유형별 상세 테스트 결과] ---\n",
      "\n",
      "--- TEST 데이터를 모든 유형을 기준으로 분리 중... ---\n",
      "  - 유형: CENSURE              | 샘플 수: 19,866\n",
      "  - 유형: ABUSE                | 샘플 수: 1,474\n",
      "  - 유형: HATE                 | 샘플 수: 9,222\n",
      "  - 유형: VIOLENCE             | 샘플 수: 2,167\n",
      "  - 유형: CRIME                | 샘플 수: 1,075\n",
      "  - 유형: SEXUAL               | 샘플 수: 2,737\n",
      "  - 유형: DISCRIMINATION       | 샘플 수: 2,664\n",
      "\n",
      "--- [유형별 테스트 결과] 유형: CENSURE ---\n",
      "총 샘플 수: 19,866\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " 비도덕성 없음 (0)       0.00      0.00      0.00         0\n",
      " 비도덕성 있음 (1)       1.00      0.90      0.95     19866\n",
      "\n",
      "    accuracy                           0.90     19866\n",
      "   macro avg       0.50      0.45      0.47     19866\n",
      "weighted avg       1.00      0.90      0.95     19866\n",
      "\n",
      "\n",
      "--- [유형별 테스트 결과] 유형: ABUSE ---\n",
      "총 샘플 수: 1,474\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " 비도덕성 없음 (0)       0.00      0.00      0.00         0\n",
      " 비도덕성 있음 (1)       1.00      0.94      0.97      1474\n",
      "\n",
      "    accuracy                           0.94      1474\n",
      "   macro avg       0.50      0.47      0.48      1474\n",
      "weighted avg       1.00      0.94      0.97      1474\n",
      "\n",
      "\n",
      "--- [유형별 테스트 결과] 유형: HATE ---\n",
      "총 샘플 수: 9,222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " 비도덕성 없음 (0)       0.00      0.00      0.00         0\n",
      " 비도덕성 있음 (1)       1.00      0.93      0.96      9222\n",
      "\n",
      "    accuracy                           0.93      9222\n",
      "   macro avg       0.50      0.46      0.48      9222\n",
      "weighted avg       1.00      0.93      0.96      9222\n",
      "\n",
      "\n",
      "--- [유형별 테스트 결과] 유형: VIOLENCE ---\n",
      "총 샘플 수: 2,167\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " 비도덕성 없음 (0)       0.00      0.00      0.00         0\n",
      " 비도덕성 있음 (1)       1.00      0.94      0.97      2167\n",
      "\n",
      "    accuracy                           0.94      2167\n",
      "   macro avg       0.50      0.47      0.48      2167\n",
      "weighted avg       1.00      0.94      0.97      2167\n",
      "\n",
      "\n",
      "--- [유형별 테스트 결과] 유형: CRIME ---\n",
      "총 샘플 수: 1,075\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " 비도덕성 없음 (0)       0.00      0.00      0.00         0\n",
      " 비도덕성 있음 (1)       1.00      0.81      0.90      1075\n",
      "\n",
      "    accuracy                           0.81      1075\n",
      "   macro avg       0.50      0.41      0.45      1075\n",
      "weighted avg       1.00      0.81      0.90      1075\n",
      "\n",
      "\n",
      "--- [유형별 테스트 결과] 유형: SEXUAL ---\n",
      "총 샘플 수: 2,737\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " 비도덕성 없음 (0)       0.00      0.00      0.00         0\n",
      " 비도덕성 있음 (1)       1.00      0.84      0.91      2737\n",
      "\n",
      "    accuracy                           0.84      2737\n",
      "   macro avg       0.50      0.42      0.46      2737\n",
      "weighted avg       1.00      0.84      0.91      2737\n",
      "\n",
      "\n",
      "--- [유형별 테스트 결과] 유형: DISCRIMINATION ---\n",
      "총 샘플 수: 2,664\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " 비도덕성 없음 (0)       0.00      0.00      0.00         0\n",
      " 비도덕성 있음 (1)       1.00      0.90      0.95      2664\n",
      "\n",
      "    accuracy                           0.90      2664\n",
      "   macro avg       0.50      0.45      0.47      2664\n",
      "weighted avg       1.00      0.90      0.95      2664\n",
      "\n",
      "\n",
      "--- 유형별 상세 테스트 종료 ---\n",
      "Saving model to ./model_checkpoint\n",
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2080 Ti\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at snunlp/KR-BERT-char16424 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./model_checkpoint\n",
      "'이런건 걍 돈없고 멍청한 전라도 여자들 겨냥한 상술이지' 은/는 폭력성이 포함된 문장입니다\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request\n",
    "\n",
    "from dataloader import Dataset\n",
    "from model import Models\n",
    "from transformers import BertTokenizer\n",
    "from utils import split_sentence\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def file_upload():\n",
    "    return render_template('load_file.html')\n",
    "    \n",
    "@app.route('/isViolence', methods = ['POST', 'GET'])\n",
    "def Predict():\n",
    "    if request.method == 'POST':\n",
    "        input_text = request.files['파일'].read().decode('utf-8') # 파일 불러오기\n",
    "        input_text = input_text.split('\\r\\n')\n",
    "        \n",
    "        if input_text == None:\n",
    "            return render_template('isViolence.html', Output = '')\n",
    "\n",
    "        sentences = split_sentence(input_text)  # 채팅 목록을 문장 단위로 분리\n",
    "        # print(sentences)\n",
    "        result = 0\n",
    "        for i, sentence in enumerate(sentences[:10000]):\n",
    "            if (i+1) % 1000 == 0:\n",
    "                print(f\"{i+1}번째 실행 중...\")\n",
    "            result += model.inference(sentence[-1])[1]\n",
    "        \n",
    "        ModelOutput = f\"해당 채팅방의 폭력성 대화 비율은 {round((result / len(sentences)) * 100, 2)}% 입니다\"\n",
    "        print(ModelOutput)\n",
    "        return render_template('isViolence.html', Output = ModelOutput)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    train_file_path = './data/train.csv'\n",
    "    valid_file_path = './data/val.csv'\n",
    "    test_file_path = './data/test.csv' \n",
    "    model_save_dir = './model_checkpoint'       # 모델 저장 디렉토리 경로\n",
    "    model_load_dir = './model_checkpoint'       # 모델 불러오기 디렉토리 경로\n",
    "\n",
    "    TRAIN_DATA_LIMIT = 200000\n",
    "\n",
    "    # # 1. 단일 Dataset 인스턴스 생성 및 데이터 로드\n",
    "    dataset = Dataset()\n",
    "    dataset.set_dataset('train', file_path=train_file_path, max_rows=TRAIN_DATA_LIMIT)\n",
    "    dataset.set_dataset('valid', file_path=valid_file_path)\n",
    "    dataset.set_dataset('test', file_path=test_file_path)\n",
    "\n",
    "    # # 2. Models 인스턴스 생성 및 Dataset 연결\n",
    "    # Models 인스턴스를 만들 때, dataset 객체를 인수로 전달합니다.\n",
    "    model = Models('krbert', num_labels = 2, dataset_instance=dataset) \n",
    "    \n",
    "    # # 3. 모델 정의 및 토크나이저 설정\n",
    "    # model.BERT()를 호출하여 모델을 생성하고, 반환된 토크나이저를 dataset에 설정합니다.\n",
    "    tokenizer_for_training = model.BERT()  \n",
    "    dataset.set_tokenizer(tokenizer_for_training)\n",
    "    \n",
    "    print(dataset.get_tokenizer())\n",
    "    \n",
    "    # # 4. 데이터 로더 생성 (이제 올바른 토크나이저를 사용)\n",
    "    train, valid, test = dataset.get_dataloader()  \n",
    "    \n",
    "    model.about_model()\n",
    "    model.train(train, valid, epochs = 2, project_title=None) \n",
    "    model.test(test)\n",
    "    model.save_model(model_save_dir)\n",
    "\n",
    "    # # 5. 추론 및 웹 서버용 모델 로드 (새로운 인스턴스 생성 및 로드)\n",
    "    # 추론용 모델도 동일한 dataset 인스턴스를 사용하도록 연결\n",
    "    web_model = Models('krbert', num_labels = 2, dataset_instance=dataset)\n",
    "    web_model.load_model(load_dir_path=model_load_dir)\n",
    "    \n",
    "    # # 6. 단일 문장 추론 테스트\n",
    "    sentence, prediction = web_model.inference('이런건 걍 돈없고 멍청한 전라도 여자들 겨냥한 상술이지')\n",
    "    print(f\"'{sentence}' 은/는 폭력성이 포함된 문장입니다\" if prediction == 1 else f\"'{sentence}' 은/는 폭력성이 포함되지 않은 문장입니다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19121076-be6e-489c-b45a-e20ea847ec86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2080 Ti\n",
      "max_length : 128\n",
      "[train]: 로드 중 파일: ./data/train.csv\n",
      "[train]: 총 1개의 파일에서 365,500개의 데이터를 성공적으로 병합했습니다.\n",
      "Train : 365500\n",
      "[valid]: 로드 중 파일: ./data/val.csv\n",
      "[valid]: 총 1개의 파일에서 40,612개의 데이터를 성공적으로 병합했습니다.\n",
      "Valid : 40612\n",
      "[test]: 로드 중 파일: ./data/test.csv\n",
      "[test]: 총 1개의 파일에서 44,998개의 데이터를 성공적으로 병합했습니다.\n",
      "Test : 44998\n",
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2080 Ti\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at snunlp/KR-BERT-char16424 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizer(name_or_path='snunlp/KR-BERT-char16424', vocab_size=16424, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n",
      "The KRBERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (16424, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch    50  of  11,422.    Elapsed: 0:00:11.\n",
      "  Batch   100  of  11,422.    Elapsed: 0:00:22.\n",
      "  Batch   150  of  11,422.    Elapsed: 0:00:34.\n",
      "  Batch   200  of  11,422.    Elapsed: 0:00:45.\n",
      "  Batch   250  of  11,422.    Elapsed: 0:00:57.\n",
      "  Batch   300  of  11,422.    Elapsed: 0:01:09.\n",
      "  Batch   350  of  11,422.    Elapsed: 0:01:20.\n",
      "  Batch   400  of  11,422.    Elapsed: 0:01:32.\n",
      "  Batch   450  of  11,422.    Elapsed: 0:01:44.\n",
      "  Batch   500  of  11,422.    Elapsed: 0:01:56.\n",
      "  Batch   550  of  11,422.    Elapsed: 0:02:08.\n",
      "  Batch   600  of  11,422.    Elapsed: 0:02:20.\n",
      "  Batch   650  of  11,422.    Elapsed: 0:02:32.\n",
      "  Batch   700  of  11,422.    Elapsed: 0:02:44.\n",
      "  Batch   750  of  11,422.    Elapsed: 0:02:56.\n",
      "  Batch   800  of  11,422.    Elapsed: 0:03:09.\n",
      "  Batch   850  of  11,422.    Elapsed: 0:03:21.\n",
      "  Batch   900  of  11,422.    Elapsed: 0:03:33.\n",
      "  Batch   950  of  11,422.    Elapsed: 0:03:45.\n",
      "  Batch 1,000  of  11,422.    Elapsed: 0:03:57.\n",
      "  Batch 1,050  of  11,422.    Elapsed: 0:04:09.\n",
      "  Batch 1,100  of  11,422.    Elapsed: 0:04:22.\n",
      "  Batch 1,150  of  11,422.    Elapsed: 0:04:34.\n",
      "  Batch 1,200  of  11,422.    Elapsed: 0:04:46.\n",
      "  Batch 1,250  of  11,422.    Elapsed: 0:04:58.\n",
      "  Batch 1,300  of  11,422.    Elapsed: 0:05:11.\n",
      "  Batch 1,350  of  11,422.    Elapsed: 0:05:23.\n",
      "  Batch 1,400  of  11,422.    Elapsed: 0:05:35.\n",
      "  Batch 1,450  of  11,422.    Elapsed: 0:05:48.\n",
      "  Batch 1,500  of  11,422.    Elapsed: 0:06:00.\n",
      "  Batch 1,550  of  11,422.    Elapsed: 0:06:12.\n",
      "  Batch 1,600  of  11,422.    Elapsed: 0:06:25.\n",
      "  Batch 1,650  of  11,422.    Elapsed: 0:06:37.\n",
      "  Batch 1,700  of  11,422.    Elapsed: 0:06:50.\n",
      "  Batch 1,750  of  11,422.    Elapsed: 0:07:02.\n",
      "  Batch 1,800  of  11,422.    Elapsed: 0:07:14.\n",
      "  Batch 1,850  of  11,422.    Elapsed: 0:07:27.\n",
      "  Batch 1,900  of  11,422.    Elapsed: 0:07:39.\n",
      "  Batch 1,950  of  11,422.    Elapsed: 0:07:52.\n",
      "  Batch 2,000  of  11,422.    Elapsed: 0:08:04.\n",
      "  Batch 2,050  of  11,422.    Elapsed: 0:08:17.\n",
      "  Batch 2,100  of  11,422.    Elapsed: 0:08:29.\n",
      "  Batch 2,150  of  11,422.    Elapsed: 0:08:41.\n",
      "  Batch 2,200  of  11,422.    Elapsed: 0:08:54.\n",
      "  Batch 2,250  of  11,422.    Elapsed: 0:09:06.\n",
      "  Batch 2,300  of  11,422.    Elapsed: 0:09:19.\n",
      "  Batch 2,350  of  11,422.    Elapsed: 0:09:31.\n",
      "  Batch 2,400  of  11,422.    Elapsed: 0:09:44.\n",
      "  Batch 2,450  of  11,422.    Elapsed: 0:09:56.\n",
      "  Batch 2,500  of  11,422.    Elapsed: 0:10:09.\n",
      "  Batch 2,550  of  11,422.    Elapsed: 0:10:21.\n",
      "  Batch 2,600  of  11,422.    Elapsed: 0:10:33.\n",
      "  Batch 2,650  of  11,422.    Elapsed: 0:10:46.\n",
      "  Batch 2,700  of  11,422.    Elapsed: 0:10:58.\n",
      "  Batch 2,750  of  11,422.    Elapsed: 0:11:11.\n",
      "  Batch 2,800  of  11,422.    Elapsed: 0:11:23.\n",
      "  Batch 2,850  of  11,422.    Elapsed: 0:11:36.\n",
      "  Batch 2,900  of  11,422.    Elapsed: 0:11:48.\n",
      "  Batch 2,950  of  11,422.    Elapsed: 0:12:01.\n",
      "  Batch 3,000  of  11,422.    Elapsed: 0:12:13.\n",
      "  Batch 3,050  of  11,422.    Elapsed: 0:12:26.\n",
      "  Batch 3,100  of  11,422.    Elapsed: 0:12:38.\n",
      "  Batch 3,150  of  11,422.    Elapsed: 0:12:51.\n",
      "  Batch 3,200  of  11,422.    Elapsed: 0:13:03.\n",
      "  Batch 3,250  of  11,422.    Elapsed: 0:13:15.\n",
      "  Batch 3,300  of  11,422.    Elapsed: 0:13:28.\n",
      "  Batch 3,350  of  11,422.    Elapsed: 0:13:40.\n",
      "  Batch 3,400  of  11,422.    Elapsed: 0:13:53.\n",
      "  Batch 3,450  of  11,422.    Elapsed: 0:14:05.\n",
      "  Batch 3,500  of  11,422.    Elapsed: 0:14:18.\n",
      "  Batch 3,550  of  11,422.    Elapsed: 0:14:30.\n",
      "  Batch 3,600  of  11,422.    Elapsed: 0:14:43.\n",
      "  Batch 3,650  of  11,422.    Elapsed: 0:14:55.\n",
      "  Batch 3,700  of  11,422.    Elapsed: 0:15:08.\n",
      "  Batch 3,750  of  11,422.    Elapsed: 0:15:20.\n",
      "  Batch 3,800  of  11,422.    Elapsed: 0:15:33.\n",
      "  Batch 3,850  of  11,422.    Elapsed: 0:15:45.\n",
      "  Batch 3,900  of  11,422.    Elapsed: 0:15:58.\n",
      "  Batch 3,950  of  11,422.    Elapsed: 0:16:10.\n",
      "  Batch 4,000  of  11,422.    Elapsed: 0:16:23.\n",
      "  Batch 4,050  of  11,422.    Elapsed: 0:16:35.\n",
      "  Batch 4,100  of  11,422.    Elapsed: 0:16:48.\n",
      "  Batch 4,150  of  11,422.    Elapsed: 0:17:00.\n",
      "  Batch 4,200  of  11,422.    Elapsed: 0:17:12.\n",
      "  Batch 4,250  of  11,422.    Elapsed: 0:17:25.\n",
      "  Batch 4,300  of  11,422.    Elapsed: 0:17:37.\n",
      "  Batch 4,350  of  11,422.    Elapsed: 0:17:50.\n",
      "  Batch 4,400  of  11,422.    Elapsed: 0:18:02.\n",
      "  Batch 4,450  of  11,422.    Elapsed: 0:18:15.\n",
      "  Batch 4,500  of  11,422.    Elapsed: 0:18:27.\n",
      "  Batch 4,550  of  11,422.    Elapsed: 0:18:40.\n",
      "  Batch 4,600  of  11,422.    Elapsed: 0:18:52.\n",
      "  Batch 4,650  of  11,422.    Elapsed: 0:19:05.\n",
      "  Batch 4,700  of  11,422.    Elapsed: 0:19:17.\n",
      "  Batch 4,750  of  11,422.    Elapsed: 0:19:30.\n",
      "  Batch 4,800  of  11,422.    Elapsed: 0:19:42.\n",
      "  Batch 4,850  of  11,422.    Elapsed: 0:19:55.\n",
      "  Batch 4,900  of  11,422.    Elapsed: 0:20:07.\n",
      "  Batch 4,950  of  11,422.    Elapsed: 0:20:19.\n",
      "  Batch 5,000  of  11,422.    Elapsed: 0:20:32.\n",
      "  Batch 5,050  of  11,422.    Elapsed: 0:20:44.\n",
      "  Batch 5,100  of  11,422.    Elapsed: 0:20:57.\n",
      "  Batch 5,150  of  11,422.    Elapsed: 0:21:09.\n",
      "  Batch 5,200  of  11,422.    Elapsed: 0:21:22.\n",
      "  Batch 5,250  of  11,422.    Elapsed: 0:21:34.\n",
      "  Batch 5,300  of  11,422.    Elapsed: 0:21:47.\n",
      "  Batch 5,350  of  11,422.    Elapsed: 0:21:59.\n",
      "  Batch 5,400  of  11,422.    Elapsed: 0:22:12.\n",
      "  Batch 5,450  of  11,422.    Elapsed: 0:22:24.\n",
      "  Batch 5,500  of  11,422.    Elapsed: 0:22:37.\n",
      "  Batch 5,550  of  11,422.    Elapsed: 0:22:49.\n",
      "  Batch 5,600  of  11,422.    Elapsed: 0:23:02.\n",
      "  Batch 5,650  of  11,422.    Elapsed: 0:23:14.\n",
      "  Batch 5,700  of  11,422.    Elapsed: 0:23:27.\n",
      "  Batch 5,750  of  11,422.    Elapsed: 0:23:39.\n",
      "  Batch 5,800  of  11,422.    Elapsed: 0:23:51.\n",
      "  Batch 5,850  of  11,422.    Elapsed: 0:24:04.\n",
      "  Batch 5,900  of  11,422.    Elapsed: 0:24:16.\n",
      "  Batch 5,950  of  11,422.    Elapsed: 0:24:29.\n",
      "  Batch 6,000  of  11,422.    Elapsed: 0:24:41.\n",
      "  Batch 6,050  of  11,422.    Elapsed: 0:24:54.\n",
      "  Batch 6,100  of  11,422.    Elapsed: 0:25:06.\n",
      "  Batch 6,150  of  11,422.    Elapsed: 0:25:19.\n",
      "  Batch 6,200  of  11,422.    Elapsed: 0:25:31.\n",
      "  Batch 6,250  of  11,422.    Elapsed: 0:25:44.\n",
      "  Batch 6,300  of  11,422.    Elapsed: 0:25:56.\n",
      "  Batch 6,350  of  11,422.    Elapsed: 0:26:09.\n",
      "  Batch 6,400  of  11,422.    Elapsed: 0:26:21.\n",
      "  Batch 6,450  of  11,422.    Elapsed: 0:26:34.\n",
      "  Batch 6,500  of  11,422.    Elapsed: 0:26:46.\n",
      "  Batch 6,550  of  11,422.    Elapsed: 0:26:58.\n",
      "  Batch 6,600  of  11,422.    Elapsed: 0:27:11.\n",
      "  Batch 6,650  of  11,422.    Elapsed: 0:27:23.\n",
      "  Batch 6,700  of  11,422.    Elapsed: 0:27:36.\n",
      "  Batch 6,750  of  11,422.    Elapsed: 0:27:48.\n",
      "  Batch 6,800  of  11,422.    Elapsed: 0:28:01.\n",
      "  Batch 6,850  of  11,422.    Elapsed: 0:28:13.\n",
      "  Batch 6,900  of  11,422.    Elapsed: 0:28:26.\n",
      "  Batch 6,950  of  11,422.    Elapsed: 0:28:38.\n",
      "  Batch 7,000  of  11,422.    Elapsed: 0:28:51.\n",
      "  Batch 7,050  of  11,422.    Elapsed: 0:29:03.\n",
      "  Batch 7,100  of  11,422.    Elapsed: 0:29:16.\n",
      "  Batch 7,150  of  11,422.    Elapsed: 0:29:28.\n",
      "  Batch 7,200  of  11,422.    Elapsed: 0:29:41.\n",
      "  Batch 7,250  of  11,422.    Elapsed: 0:29:53.\n",
      "  Batch 7,300  of  11,422.    Elapsed: 0:30:06.\n",
      "  Batch 7,350  of  11,422.    Elapsed: 0:30:18.\n",
      "  Batch 7,400  of  11,422.    Elapsed: 0:30:30.\n",
      "  Batch 7,450  of  11,422.    Elapsed: 0:30:43.\n",
      "  Batch 7,500  of  11,422.    Elapsed: 0:30:55.\n",
      "  Batch 7,550  of  11,422.    Elapsed: 0:31:08.\n",
      "  Batch 7,600  of  11,422.    Elapsed: 0:31:20.\n",
      "  Batch 7,650  of  11,422.    Elapsed: 0:31:33.\n",
      "  Batch 7,700  of  11,422.    Elapsed: 0:31:45.\n",
      "  Batch 7,750  of  11,422.    Elapsed: 0:31:58.\n",
      "  Batch 7,800  of  11,422.    Elapsed: 0:32:10.\n",
      "  Batch 7,850  of  11,422.    Elapsed: 0:32:23.\n",
      "  Batch 7,900  of  11,422.    Elapsed: 0:32:35.\n",
      "  Batch 7,950  of  11,422.    Elapsed: 0:32:48.\n",
      "  Batch 8,000  of  11,422.    Elapsed: 0:33:00.\n",
      "  Batch 8,050  of  11,422.    Elapsed: 0:33:13.\n",
      "  Batch 8,100  of  11,422.    Elapsed: 0:33:25.\n",
      "  Batch 8,150  of  11,422.    Elapsed: 0:33:38.\n",
      "  Batch 8,200  of  11,422.    Elapsed: 0:33:50.\n",
      "  Batch 8,250  of  11,422.    Elapsed: 0:34:02.\n",
      "  Batch 8,300  of  11,422.    Elapsed: 0:34:15.\n",
      "  Batch 8,350  of  11,422.    Elapsed: 0:34:27.\n",
      "  Batch 8,400  of  11,422.    Elapsed: 0:34:40.\n",
      "  Batch 8,450  of  11,422.    Elapsed: 0:34:52.\n",
      "  Batch 8,500  of  11,422.    Elapsed: 0:35:05.\n",
      "  Batch 8,550  of  11,422.    Elapsed: 0:35:17.\n",
      "  Batch 8,600  of  11,422.    Elapsed: 0:35:30.\n",
      "  Batch 8,650  of  11,422.    Elapsed: 0:35:42.\n",
      "  Batch 8,700  of  11,422.    Elapsed: 0:35:55.\n",
      "  Batch 8,750  of  11,422.    Elapsed: 0:36:07.\n",
      "  Batch 8,800  of  11,422.    Elapsed: 0:36:20.\n",
      "  Batch 8,850  of  11,422.    Elapsed: 0:36:32.\n",
      "  Batch 8,900  of  11,422.    Elapsed: 0:36:45.\n",
      "  Batch 8,950  of  11,422.    Elapsed: 0:36:57.\n",
      "  Batch 9,000  of  11,422.    Elapsed: 0:37:10.\n",
      "  Batch 9,050  of  11,422.    Elapsed: 0:37:22.\n",
      "  Batch 9,100  of  11,422.    Elapsed: 0:37:34.\n",
      "  Batch 9,150  of  11,422.    Elapsed: 0:37:47.\n",
      "  Batch 9,200  of  11,422.    Elapsed: 0:37:59.\n",
      "  Batch 9,250  of  11,422.    Elapsed: 0:38:12.\n",
      "  Batch 9,300  of  11,422.    Elapsed: 0:38:24.\n",
      "  Batch 9,350  of  11,422.    Elapsed: 0:38:37.\n",
      "  Batch 9,400  of  11,422.    Elapsed: 0:38:49.\n",
      "  Batch 9,450  of  11,422.    Elapsed: 0:39:02.\n",
      "  Batch 9,500  of  11,422.    Elapsed: 0:39:14.\n",
      "  Batch 9,550  of  11,422.    Elapsed: 0:39:27.\n",
      "  Batch 9,600  of  11,422.    Elapsed: 0:39:39.\n",
      "  Batch 9,650  of  11,422.    Elapsed: 0:39:52.\n",
      "  Batch 9,700  of  11,422.    Elapsed: 0:40:04.\n",
      "  Batch 9,750  of  11,422.    Elapsed: 0:40:17.\n",
      "  Batch 9,800  of  11,422.    Elapsed: 0:40:29.\n",
      "  Batch 9,850  of  11,422.    Elapsed: 0:40:41.\n",
      "  Batch 9,900  of  11,422.    Elapsed: 0:40:54.\n",
      "  Batch 9,950  of  11,422.    Elapsed: 0:41:06.\n",
      "  Batch 10,000  of  11,422.    Elapsed: 0:41:19.\n",
      "  Batch 10,050  of  11,422.    Elapsed: 0:41:31.\n",
      "  Batch 10,100  of  11,422.    Elapsed: 0:41:44.\n",
      "  Batch 10,150  of  11,422.    Elapsed: 0:41:56.\n",
      "  Batch 10,200  of  11,422.    Elapsed: 0:42:09.\n",
      "  Batch 10,250  of  11,422.    Elapsed: 0:42:21.\n",
      "  Batch 10,300  of  11,422.    Elapsed: 0:42:34.\n",
      "  Batch 10,350  of  11,422.    Elapsed: 0:42:46.\n",
      "  Batch 10,400  of  11,422.    Elapsed: 0:42:59.\n",
      "  Batch 10,450  of  11,422.    Elapsed: 0:43:11.\n",
      "  Batch 10,500  of  11,422.    Elapsed: 0:43:23.\n",
      "  Batch 10,550  of  11,422.    Elapsed: 0:43:36.\n",
      "  Batch 10,600  of  11,422.    Elapsed: 0:43:48.\n",
      "  Batch 10,650  of  11,422.    Elapsed: 0:44:01.\n",
      "  Batch 10,700  of  11,422.    Elapsed: 0:44:13.\n",
      "  Batch 10,750  of  11,422.    Elapsed: 0:44:26.\n",
      "  Batch 10,800  of  11,422.    Elapsed: 0:44:38.\n",
      "  Batch 10,850  of  11,422.    Elapsed: 0:44:51.\n",
      "  Batch 10,900  of  11,422.    Elapsed: 0:45:03.\n",
      "  Batch 10,950  of  11,422.    Elapsed: 0:45:16.\n",
      "  Batch 11,000  of  11,422.    Elapsed: 0:45:28.\n",
      "  Batch 11,050  of  11,422.    Elapsed: 0:45:40.\n",
      "  Batch 11,100  of  11,422.    Elapsed: 0:45:53.\n",
      "  Batch 11,150  of  11,422.    Elapsed: 0:46:05.\n",
      "  Batch 11,200  of  11,422.    Elapsed: 0:46:18.\n",
      "  Batch 11,250  of  11,422.    Elapsed: 0:46:30.\n",
      "  Batch 11,300  of  11,422.    Elapsed: 0:46:43.\n",
      "  Batch 11,350  of  11,422.    Elapsed: 0:46:55.\n",
      "  Batch 11,400  of  11,422.    Elapsed: 0:47:08.\n",
      "\n",
      "  Average training loss: 0.38\n",
      "  Training epcoh took: 0:47:13\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.85\n",
      "  Validation Loss: 0.35\n",
      "  Validation took: 0:01:38\n",
      "\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch    50  of  11,422.    Elapsed: 0:00:12.\n",
      "  Batch   100  of  11,422.    Elapsed: 0:00:25.\n",
      "  Batch   150  of  11,422.    Elapsed: 0:00:37.\n",
      "  Batch   200  of  11,422.    Elapsed: 0:00:50.\n",
      "  Batch   250  of  11,422.    Elapsed: 0:01:02.\n",
      "  Batch   300  of  11,422.    Elapsed: 0:01:15.\n",
      "  Batch   350  of  11,422.    Elapsed: 0:01:27.\n",
      "  Batch   400  of  11,422.    Elapsed: 0:01:39.\n",
      "  Batch   450  of  11,422.    Elapsed: 0:01:52.\n",
      "  Batch   500  of  11,422.    Elapsed: 0:02:04.\n",
      "  Batch   550  of  11,422.    Elapsed: 0:02:17.\n",
      "  Batch   600  of  11,422.    Elapsed: 0:02:29.\n",
      "  Batch   650  of  11,422.    Elapsed: 0:02:42.\n",
      "  Batch   700  of  11,422.    Elapsed: 0:02:54.\n",
      "  Batch   750  of  11,422.    Elapsed: 0:03:07.\n",
      "  Batch   800  of  11,422.    Elapsed: 0:03:19.\n",
      "  Batch   850  of  11,422.    Elapsed: 0:03:32.\n",
      "  Batch   900  of  11,422.    Elapsed: 0:03:44.\n",
      "  Batch   950  of  11,422.    Elapsed: 0:03:56.\n",
      "  Batch 1,000  of  11,422.    Elapsed: 0:04:09.\n",
      "  Batch 1,050  of  11,422.    Elapsed: 0:04:21.\n",
      "  Batch 1,100  of  11,422.    Elapsed: 0:04:34.\n",
      "  Batch 1,150  of  11,422.    Elapsed: 0:04:46.\n",
      "  Batch 1,200  of  11,422.    Elapsed: 0:04:59.\n",
      "  Batch 1,250  of  11,422.    Elapsed: 0:05:11.\n",
      "  Batch 1,300  of  11,422.    Elapsed: 0:05:24.\n",
      "  Batch 1,350  of  11,422.    Elapsed: 0:05:36.\n",
      "  Batch 1,400  of  11,422.    Elapsed: 0:05:48.\n",
      "  Batch 1,450  of  11,422.    Elapsed: 0:06:01.\n",
      "  Batch 1,500  of  11,422.    Elapsed: 0:06:13.\n",
      "  Batch 1,550  of  11,422.    Elapsed: 0:06:26.\n",
      "  Batch 1,600  of  11,422.    Elapsed: 0:06:38.\n",
      "  Batch 1,650  of  11,422.    Elapsed: 0:06:51.\n",
      "  Batch 1,700  of  11,422.    Elapsed: 0:07:03.\n",
      "  Batch 1,750  of  11,422.    Elapsed: 0:07:16.\n",
      "  Batch 1,800  of  11,422.    Elapsed: 0:07:28.\n",
      "  Batch 1,850  of  11,422.    Elapsed: 0:07:40.\n",
      "  Batch 1,900  of  11,422.    Elapsed: 0:07:53.\n",
      "  Batch 1,950  of  11,422.    Elapsed: 0:08:05.\n",
      "  Batch 2,000  of  11,422.    Elapsed: 0:08:18.\n",
      "  Batch 2,050  of  11,422.    Elapsed: 0:08:30.\n",
      "  Batch 2,100  of  11,422.    Elapsed: 0:08:43.\n",
      "  Batch 2,150  of  11,422.    Elapsed: 0:08:55.\n",
      "  Batch 2,200  of  11,422.    Elapsed: 0:09:08.\n",
      "  Batch 2,250  of  11,422.    Elapsed: 0:09:20.\n",
      "  Batch 2,300  of  11,422.    Elapsed: 0:09:32.\n",
      "  Batch 2,350  of  11,422.    Elapsed: 0:09:45.\n",
      "  Batch 2,400  of  11,422.    Elapsed: 0:09:57.\n",
      "  Batch 2,450  of  11,422.    Elapsed: 0:10:10.\n",
      "  Batch 2,500  of  11,422.    Elapsed: 0:10:22.\n",
      "  Batch 2,550  of  11,422.    Elapsed: 0:10:35.\n",
      "  Batch 2,600  of  11,422.    Elapsed: 0:10:47.\n",
      "  Batch 2,650  of  11,422.    Elapsed: 0:11:00.\n",
      "  Batch 2,700  of  11,422.    Elapsed: 0:11:12.\n",
      "  Batch 2,750  of  11,422.    Elapsed: 0:11:24.\n",
      "  Batch 2,800  of  11,422.    Elapsed: 0:11:37.\n",
      "  Batch 2,850  of  11,422.    Elapsed: 0:11:49.\n",
      "  Batch 2,900  of  11,422.    Elapsed: 0:12:02.\n",
      "  Batch 2,950  of  11,422.    Elapsed: 0:12:14.\n",
      "  Batch 3,000  of  11,422.    Elapsed: 0:12:27.\n",
      "  Batch 3,050  of  11,422.    Elapsed: 0:12:39.\n",
      "  Batch 3,100  of  11,422.    Elapsed: 0:12:52.\n",
      "  Batch 3,150  of  11,422.    Elapsed: 0:13:04.\n",
      "  Batch 3,200  of  11,422.    Elapsed: 0:13:16.\n",
      "  Batch 3,250  of  11,422.    Elapsed: 0:13:29.\n",
      "  Batch 3,300  of  11,422.    Elapsed: 0:13:41.\n",
      "  Batch 3,350  of  11,422.    Elapsed: 0:13:54.\n",
      "  Batch 3,400  of  11,422.    Elapsed: 0:14:06.\n",
      "  Batch 3,450  of  11,422.    Elapsed: 0:14:19.\n",
      "  Batch 3,500  of  11,422.    Elapsed: 0:14:31.\n",
      "  Batch 3,550  of  11,422.    Elapsed: 0:14:44.\n",
      "  Batch 3,600  of  11,422.    Elapsed: 0:14:56.\n",
      "  Batch 3,650  of  11,422.    Elapsed: 0:15:08.\n",
      "  Batch 3,700  of  11,422.    Elapsed: 0:15:21.\n",
      "  Batch 3,750  of  11,422.    Elapsed: 0:15:33.\n",
      "  Batch 3,800  of  11,422.    Elapsed: 0:15:46.\n",
      "  Batch 3,850  of  11,422.    Elapsed: 0:15:58.\n",
      "  Batch 3,900  of  11,422.    Elapsed: 0:16:11.\n",
      "  Batch 3,950  of  11,422.    Elapsed: 0:16:23.\n",
      "  Batch 4,000  of  11,422.    Elapsed: 0:16:36.\n",
      "  Batch 4,050  of  11,422.    Elapsed: 0:16:48.\n",
      "  Batch 4,100  of  11,422.    Elapsed: 0:17:01.\n",
      "  Batch 4,150  of  11,422.    Elapsed: 0:17:13.\n",
      "  Batch 4,200  of  11,422.    Elapsed: 0:17:25.\n",
      "  Batch 4,250  of  11,422.    Elapsed: 0:17:38.\n",
      "  Batch 4,300  of  11,422.    Elapsed: 0:17:50.\n",
      "  Batch 4,350  of  11,422.    Elapsed: 0:18:03.\n",
      "  Batch 4,400  of  11,422.    Elapsed: 0:18:15.\n",
      "  Batch 4,450  of  11,422.    Elapsed: 0:18:28.\n",
      "  Batch 4,500  of  11,422.    Elapsed: 0:18:40.\n",
      "  Batch 4,550  of  11,422.    Elapsed: 0:18:53.\n",
      "  Batch 4,600  of  11,422.    Elapsed: 0:19:05.\n",
      "  Batch 4,650  of  11,422.    Elapsed: 0:19:18.\n",
      "  Batch 4,700  of  11,422.    Elapsed: 0:19:30.\n",
      "  Batch 4,750  of  11,422.    Elapsed: 0:19:43.\n",
      "  Batch 4,800  of  11,422.    Elapsed: 0:19:55.\n",
      "  Batch 4,850  of  11,422.    Elapsed: 0:20:07.\n",
      "  Batch 4,900  of  11,422.    Elapsed: 0:20:20.\n",
      "  Batch 4,950  of  11,422.    Elapsed: 0:20:32.\n",
      "  Batch 5,000  of  11,422.    Elapsed: 0:20:45.\n",
      "  Batch 5,050  of  11,422.    Elapsed: 0:20:57.\n",
      "  Batch 5,100  of  11,422.    Elapsed: 0:21:10.\n",
      "  Batch 5,150  of  11,422.    Elapsed: 0:21:22.\n",
      "  Batch 5,200  of  11,422.    Elapsed: 0:21:35.\n",
      "  Batch 5,250  of  11,422.    Elapsed: 0:21:47.\n",
      "  Batch 5,300  of  11,422.    Elapsed: 0:22:00.\n",
      "  Batch 5,350  of  11,422.    Elapsed: 0:22:12.\n",
      "  Batch 5,400  of  11,422.    Elapsed: 0:22:25.\n",
      "  Batch 5,450  of  11,422.    Elapsed: 0:22:37.\n",
      "  Batch 5,500  of  11,422.    Elapsed: 0:22:50.\n",
      "  Batch 5,550  of  11,422.    Elapsed: 0:23:02.\n",
      "  Batch 5,600  of  11,422.    Elapsed: 0:23:14.\n",
      "  Batch 5,650  of  11,422.    Elapsed: 0:23:27.\n",
      "  Batch 5,700  of  11,422.    Elapsed: 0:23:39.\n",
      "  Batch 5,750  of  11,422.    Elapsed: 0:23:52.\n",
      "  Batch 5,800  of  11,422.    Elapsed: 0:24:04.\n",
      "  Batch 5,850  of  11,422.    Elapsed: 0:24:17.\n",
      "  Batch 5,900  of  11,422.    Elapsed: 0:24:29.\n",
      "  Batch 5,950  of  11,422.    Elapsed: 0:24:42.\n",
      "  Batch 6,000  of  11,422.    Elapsed: 0:24:54.\n",
      "  Batch 6,050  of  11,422.    Elapsed: 0:25:07.\n",
      "  Batch 6,100  of  11,422.    Elapsed: 0:25:19.\n",
      "  Batch 6,150  of  11,422.    Elapsed: 0:25:32.\n",
      "  Batch 6,200  of  11,422.    Elapsed: 0:25:44.\n",
      "  Batch 6,250  of  11,422.    Elapsed: 0:25:57.\n",
      "  Batch 6,300  of  11,422.    Elapsed: 0:26:09.\n",
      "  Batch 6,350  of  11,422.    Elapsed: 0:26:22.\n",
      "  Batch 6,400  of  11,422.    Elapsed: 0:26:34.\n",
      "  Batch 6,450  of  11,422.    Elapsed: 0:26:46.\n",
      "  Batch 6,500  of  11,422.    Elapsed: 0:26:59.\n",
      "  Batch 6,550  of  11,422.    Elapsed: 0:27:11.\n",
      "  Batch 6,600  of  11,422.    Elapsed: 0:27:24.\n",
      "  Batch 6,650  of  11,422.    Elapsed: 0:27:36.\n",
      "  Batch 6,700  of  11,422.    Elapsed: 0:27:49.\n",
      "  Batch 6,750  of  11,422.    Elapsed: 0:28:01.\n",
      "  Batch 6,800  of  11,422.    Elapsed: 0:28:14.\n",
      "  Batch 6,850  of  11,422.    Elapsed: 0:28:26.\n",
      "  Batch 6,900  of  11,422.    Elapsed: 0:28:39.\n",
      "  Batch 6,950  of  11,422.    Elapsed: 0:28:51.\n",
      "  Batch 7,000  of  11,422.    Elapsed: 0:29:04.\n",
      "  Batch 7,050  of  11,422.    Elapsed: 0:29:16.\n",
      "  Batch 7,100  of  11,422.    Elapsed: 0:29:29.\n",
      "  Batch 7,150  of  11,422.    Elapsed: 0:29:41.\n",
      "  Batch 7,200  of  11,422.    Elapsed: 0:29:53.\n",
      "  Batch 7,250  of  11,422.    Elapsed: 0:30:06.\n",
      "  Batch 7,300  of  11,422.    Elapsed: 0:30:18.\n",
      "  Batch 7,350  of  11,422.    Elapsed: 0:30:31.\n",
      "  Batch 7,400  of  11,422.    Elapsed: 0:30:43.\n",
      "  Batch 7,450  of  11,422.    Elapsed: 0:30:56.\n",
      "  Batch 7,500  of  11,422.    Elapsed: 0:31:08.\n",
      "  Batch 7,550  of  11,422.    Elapsed: 0:31:21.\n",
      "  Batch 7,600  of  11,422.    Elapsed: 0:31:33.\n",
      "  Batch 7,650  of  11,422.    Elapsed: 0:31:46.\n",
      "  Batch 7,700  of  11,422.    Elapsed: 0:31:58.\n",
      "  Batch 7,750  of  11,422.    Elapsed: 0:32:11.\n",
      "  Batch 7,800  of  11,422.    Elapsed: 0:32:23.\n",
      "  Batch 7,850  of  11,422.    Elapsed: 0:32:36.\n",
      "  Batch 7,900  of  11,422.    Elapsed: 0:32:48.\n",
      "  Batch 7,950  of  11,422.    Elapsed: 0:33:01.\n",
      "  Batch 8,000  of  11,422.    Elapsed: 0:33:13.\n",
      "  Batch 8,050  of  11,422.    Elapsed: 0:33:25.\n",
      "  Batch 8,100  of  11,422.    Elapsed: 0:33:38.\n",
      "  Batch 8,150  of  11,422.    Elapsed: 0:33:50.\n",
      "  Batch 8,200  of  11,422.    Elapsed: 0:34:03.\n",
      "  Batch 8,250  of  11,422.    Elapsed: 0:34:15.\n",
      "  Batch 8,300  of  11,422.    Elapsed: 0:34:28.\n",
      "  Batch 8,350  of  11,422.    Elapsed: 0:34:40.\n",
      "  Batch 8,400  of  11,422.    Elapsed: 0:34:53.\n",
      "  Batch 8,450  of  11,422.    Elapsed: 0:35:05.\n",
      "  Batch 8,500  of  11,422.    Elapsed: 0:35:18.\n",
      "  Batch 8,550  of  11,422.    Elapsed: 0:35:30.\n",
      "  Batch 8,600  of  11,422.    Elapsed: 0:35:43.\n",
      "  Batch 8,650  of  11,422.    Elapsed: 0:35:55.\n",
      "  Batch 8,700  of  11,422.    Elapsed: 0:36:08.\n",
      "  Batch 8,750  of  11,422.    Elapsed: 0:36:20.\n",
      "  Batch 8,800  of  11,422.    Elapsed: 0:36:32.\n",
      "  Batch 8,850  of  11,422.    Elapsed: 0:36:45.\n",
      "  Batch 8,900  of  11,422.    Elapsed: 0:36:57.\n",
      "  Batch 8,950  of  11,422.    Elapsed: 0:37:10.\n",
      "  Batch 9,000  of  11,422.    Elapsed: 0:37:22.\n",
      "  Batch 9,050  of  11,422.    Elapsed: 0:37:35.\n",
      "  Batch 9,100  of  11,422.    Elapsed: 0:37:47.\n",
      "  Batch 9,150  of  11,422.    Elapsed: 0:38:00.\n",
      "  Batch 9,200  of  11,422.    Elapsed: 0:38:12.\n",
      "  Batch 9,250  of  11,422.    Elapsed: 0:38:25.\n",
      "  Batch 9,300  of  11,422.    Elapsed: 0:38:37.\n",
      "  Batch 9,350  of  11,422.    Elapsed: 0:38:50.\n",
      "  Batch 9,400  of  11,422.    Elapsed: 0:39:02.\n",
      "  Batch 9,450  of  11,422.    Elapsed: 0:39:15.\n",
      "  Batch 9,500  of  11,422.    Elapsed: 0:39:27.\n",
      "  Batch 9,550  of  11,422.    Elapsed: 0:39:40.\n",
      "  Batch 9,600  of  11,422.    Elapsed: 0:39:52.\n",
      "  Batch 9,650  of  11,422.    Elapsed: 0:40:04.\n",
      "  Batch 9,700  of  11,422.    Elapsed: 0:40:17.\n",
      "  Batch 9,750  of  11,422.    Elapsed: 0:40:29.\n",
      "  Batch 9,800  of  11,422.    Elapsed: 0:40:42.\n",
      "  Batch 9,850  of  11,422.    Elapsed: 0:40:54.\n",
      "  Batch 9,900  of  11,422.    Elapsed: 0:41:07.\n",
      "  Batch 9,950  of  11,422.    Elapsed: 0:41:19.\n",
      "  Batch 10,000  of  11,422.    Elapsed: 0:41:32.\n",
      "  Batch 10,050  of  11,422.    Elapsed: 0:41:44.\n",
      "  Batch 10,100  of  11,422.    Elapsed: 0:41:57.\n",
      "  Batch 10,150  of  11,422.    Elapsed: 0:42:09.\n",
      "  Batch 10,200  of  11,422.    Elapsed: 0:42:22.\n",
      "  Batch 10,250  of  11,422.    Elapsed: 0:42:34.\n",
      "  Batch 10,300  of  11,422.    Elapsed: 0:42:47.\n",
      "  Batch 10,350  of  11,422.    Elapsed: 0:42:59.\n",
      "  Batch 10,400  of  11,422.    Elapsed: 0:43:12.\n",
      "  Batch 10,450  of  11,422.    Elapsed: 0:43:24.\n",
      "  Batch 10,500  of  11,422.    Elapsed: 0:43:36.\n",
      "  Batch 10,550  of  11,422.    Elapsed: 0:43:49.\n",
      "  Batch 10,600  of  11,422.    Elapsed: 0:44:01.\n",
      "  Batch 10,650  of  11,422.    Elapsed: 0:44:14.\n",
      "  Batch 10,700  of  11,422.    Elapsed: 0:44:26.\n",
      "  Batch 10,750  of  11,422.    Elapsed: 0:44:39.\n",
      "  Batch 10,800  of  11,422.    Elapsed: 0:44:51.\n",
      "  Batch 10,850  of  11,422.    Elapsed: 0:45:04.\n",
      "  Batch 10,900  of  11,422.    Elapsed: 0:45:16.\n",
      "  Batch 10,950  of  11,422.    Elapsed: 0:45:29.\n",
      "  Batch 11,000  of  11,422.    Elapsed: 0:45:41.\n",
      "  Batch 11,050  of  11,422.    Elapsed: 0:45:54.\n",
      "  Batch 11,100  of  11,422.    Elapsed: 0:46:06.\n",
      "  Batch 11,150  of  11,422.    Elapsed: 0:46:19.\n",
      "  Batch 11,200  of  11,422.    Elapsed: 0:46:31.\n",
      "  Batch 11,250  of  11,422.    Elapsed: 0:46:44.\n",
      "  Batch 11,300  of  11,422.    Elapsed: 0:46:56.\n",
      "  Batch 11,350  of  11,422.    Elapsed: 0:47:09.\n",
      "  Batch 11,400  of  11,422.    Elapsed: 0:47:21.\n",
      "\n",
      "  Average training loss: 0.29\n",
      "  Training epcoh took: 0:47:26\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.86\n",
      "  Validation Loss: 0.36\n",
      "  Validation took: 0:01:38\n",
      "\n",
      "Training complete!\n",
      "Total training took 1:37:56 (h:mm:ss)\n",
      "Predicting labels for 1,407 total test sentences...\n",
      "\n",
      "--- [전체 데이터셋] 테스트 결과 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " 비도덕성 없음 (0)       0.86      0.91      0.88     18987\n",
      " 비도덕성 있음 (1)       0.93      0.89      0.91     26011\n",
      "\n",
      "    accuracy                           0.90     44998\n",
      "   macro avg       0.89      0.90      0.89     44998\n",
      "weighted avg       0.90      0.90      0.90     44998\n",
      "\n",
      "    DONE (Total Test).\n",
      "\n",
      "\n",
      "--- [유형별 상세 테스트 결과] ---\n",
      "\n",
      "--- TEST 데이터를 모든 유형을 기준으로 분리 중... ---\n",
      "  - 유형: CENSURE              | 샘플 수: 19,866\n",
      "  - 유형: ABUSE                | 샘플 수: 1,474\n",
      "  - 유형: HATE                 | 샘플 수: 9,222\n",
      "  - 유형: VIOLENCE             | 샘플 수: 2,167\n",
      "  - 유형: CRIME                | 샘플 수: 1,075\n",
      "  - 유형: SEXUAL               | 샘플 수: 2,737\n",
      "  - 유형: DISCRIMINATION       | 샘플 수: 2,664\n",
      "\n",
      "--- [유형별 테스트 결과] 유형: CENSURE ---\n",
      "총 샘플 수: 19,866\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " 비도덕성 없음 (0)       0.00      0.00      0.00         0\n",
      " 비도덕성 있음 (1)       1.00      0.90      0.95     19866\n",
      "\n",
      "    accuracy                           0.90     19866\n",
      "   macro avg       0.50      0.45      0.47     19866\n",
      "weighted avg       1.00      0.90      0.95     19866\n",
      "\n",
      "\n",
      "--- [유형별 테스트 결과] 유형: ABUSE ---\n",
      "총 샘플 수: 1,474\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " 비도덕성 없음 (0)       0.00      0.00      0.00         0\n",
      " 비도덕성 있음 (1)       1.00      0.96      0.98      1474\n",
      "\n",
      "    accuracy                           0.96      1474\n",
      "   macro avg       0.50      0.48      0.49      1474\n",
      "weighted avg       1.00      0.96      0.98      1474\n",
      "\n",
      "\n",
      "--- [유형별 테스트 결과] 유형: HATE ---\n",
      "총 샘플 수: 9,222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " 비도덕성 없음 (0)       0.00      0.00      0.00         0\n",
      " 비도덕성 있음 (1)       1.00      0.93      0.96      9222\n",
      "\n",
      "    accuracy                           0.93      9222\n",
      "   macro avg       0.50      0.47      0.48      9222\n",
      "weighted avg       1.00      0.93      0.96      9222\n",
      "\n",
      "\n",
      "--- [유형별 테스트 결과] 유형: VIOLENCE ---\n",
      "총 샘플 수: 2,167\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " 비도덕성 없음 (0)       0.00      0.00      0.00         0\n",
      " 비도덕성 있음 (1)       1.00      0.94      0.97      2167\n",
      "\n",
      "    accuracy                           0.94      2167\n",
      "   macro avg       0.50      0.47      0.48      2167\n",
      "weighted avg       1.00      0.94      0.97      2167\n",
      "\n",
      "\n",
      "--- [유형별 테스트 결과] 유형: CRIME ---\n",
      "총 샘플 수: 1,075\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " 비도덕성 없음 (0)       0.00      0.00      0.00         0\n",
      " 비도덕성 있음 (1)       1.00      0.83      0.91      1075\n",
      "\n",
      "    accuracy                           0.83      1075\n",
      "   macro avg       0.50      0.41      0.45      1075\n",
      "weighted avg       1.00      0.83      0.91      1075\n",
      "\n",
      "\n",
      "--- [유형별 테스트 결과] 유형: SEXUAL ---\n",
      "총 샘플 수: 2,737\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " 비도덕성 없음 (0)       0.00      0.00      0.00         0\n",
      " 비도덕성 있음 (1)       1.00      0.86      0.93      2737\n",
      "\n",
      "    accuracy                           0.86      2737\n",
      "   macro avg       0.50      0.43      0.46      2737\n",
      "weighted avg       1.00      0.86      0.93      2737\n",
      "\n",
      "\n",
      "--- [유형별 테스트 결과] 유형: DISCRIMINATION ---\n",
      "총 샘플 수: 2,664\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " 비도덕성 없음 (0)       0.00      0.00      0.00         0\n",
      " 비도덕성 있음 (1)       1.00      0.91      0.95      2664\n",
      "\n",
      "    accuracy                           0.91      2664\n",
      "   macro avg       0.50      0.46      0.48      2664\n",
      "weighted avg       1.00      0.91      0.95      2664\n",
      "\n",
      "\n",
      "--- 유형별 상세 테스트 종료 ---\n",
      "Saving model to ./model_checkpoint\n",
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2080 Ti\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at snunlp/KR-BERT-char16424 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./model_checkpoint\n",
      "'이런건 걍 돈없고 멍청한 전라도 여자들 겨냥한 상술이지' 은/는 폭력성이 포함된 문장입니다\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request\n",
    "\n",
    "from dataloader import Dataset\n",
    "from model import Models\n",
    "from transformers import BertTokenizer\n",
    "from utils import split_sentence\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def file_upload():\n",
    "    return render_template('load_file.html')\n",
    "    \n",
    "@app.route('/isViolence', methods = ['POST', 'GET'])\n",
    "def Predict():\n",
    "    if request.method == 'POST':\n",
    "        input_text = request.files['파일'].read().decode('utf-8') # 파일 불러오기\n",
    "        input_text = input_text.split('\\r\\n')\n",
    "        \n",
    "        if input_text == None:\n",
    "            return render_template('isViolence.html', Output = '')\n",
    "\n",
    "        sentences = split_sentence(input_text)  # 채팅 목록을 문장 단위로 분리\n",
    "        # print(sentences)\n",
    "        result = 0\n",
    "        for i, sentence in enumerate(sentences[:10000]):\n",
    "            if (i+1) % 1000 == 0:\n",
    "                print(f\"{i+1}번째 실행 중...\")\n",
    "            result += model.inference(sentence[-1])[1]\n",
    "        \n",
    "        ModelOutput = f\"해당 채팅방의 폭력성 대화 비율은 {round((result / len(sentences)) * 100, 2)}% 입니다\"\n",
    "        print(ModelOutput)\n",
    "        return render_template('isViolence.html', Output = ModelOutput)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    train_file_path = './data/train.csv'\n",
    "    valid_file_path = './data/val.csv'\n",
    "    test_file_path = './data/test.csv' \n",
    "    model_save_dir = './model_checkpoint'       # 모델 저장 디렉토리 경로\n",
    "    model_load_dir = './model_checkpoint'       # 모델 불러오기 디렉토리 경로\n",
    "\n",
    "    TRAIN_DATA_LIMIT = 365500\n",
    "\n",
    "    # # 1. 단일 Dataset 인스턴스 생성 및 데이터 로드\n",
    "    dataset = Dataset()\n",
    "    dataset.set_dataset('train', file_path=train_file_path, max_rows=TRAIN_DATA_LIMIT)\n",
    "    dataset.set_dataset('valid', file_path=valid_file_path)\n",
    "    dataset.set_dataset('test', file_path=test_file_path)\n",
    "\n",
    "    # # 2. Models 인스턴스 생성 및 Dataset 연결\n",
    "    # Models 인스턴스를 만들 때, dataset 객체를 인수로 전달합니다.\n",
    "    model = Models('krbert', num_labels = 2, dataset_instance=dataset) \n",
    "    \n",
    "    # # 3. 모델 정의 및 토크나이저 설정\n",
    "    # model.BERT()를 호출하여 모델을 생성하고, 반환된 토크나이저를 dataset에 설정합니다.\n",
    "    tokenizer_for_training = model.BERT()  \n",
    "    dataset.set_tokenizer(tokenizer_for_training)\n",
    "    \n",
    "    print(dataset.get_tokenizer())\n",
    "    \n",
    "    # # 4. 데이터 로더 생성 (이제 올바른 토크나이저를 사용)\n",
    "    train, valid, test = dataset.get_dataloader()  \n",
    "    \n",
    "    model.about_model()\n",
    "    model.train(train, valid, epochs = 2, project_title=None) \n",
    "    model.test(test)\n",
    "    model.save_model(model_save_dir)\n",
    "\n",
    "    # # 5. 추론 및 웹 서버용 모델 로드 (새로운 인스턴스 생성 및 로드)\n",
    "    # 추론용 모델도 동일한 dataset 인스턴스를 사용하도록 연결\n",
    "    web_model = Models('krbert', num_labels = 2, dataset_instance=dataset)\n",
    "    web_model.load_model(load_dir_path=model_load_dir)\n",
    "    \n",
    "    # # 6. 단일 문장 추론 테스트\n",
    "    sentence, prediction = web_model.inference('이런건 걍 돈없고 멍청한 전라도 여자들 겨냥한 상술이지')\n",
    "    print(f\"'{sentence}' 은/는 폭력성이 포함된 문장입니다\" if prediction == 1 else f\"'{sentence}' 은/는 폭력성이 포함되지 않은 문장입니다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63c0d2f-c0b7-41a3-94e9-8bb2ddac7072",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35fdb0f-45ff-49f1-9a7c-fd3a27a4cb69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
