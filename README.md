# KR-BERT 활용한 한국어 대화 텍스트 내의 폭력성 유무 분류
이화여자대학교 컴퓨터공학과 2271059 좌연주

## 요약
본 연구는 증가하는 사이버 폭력의 심각성에 대응하기 위해, 한국어 대화 텍스트 내의 폭력성 유무를 문장 단위로 자동 분류하는 KR-BERT 기반의 자연어 처리(NLP) 모델을 개발하고 성능을 분석하는 것을 목표로 한다. 기존 모델의 성능을 검증하고, LoRA (Low-Rank Adaptation) 기법을 적용하여 모델의 파인튜닝 효율성을 분석했다. 실험 결과, 모델은 7가지 폭력성 유형 분류에서 전반적으로 높은 F1-Score (0.90)를 달성했으나, 파인튜닝 후 전체 F1-Score가 0.72로 크게 하락하는 현상이 관찰되었습니다. 이는 학습 데이터의 양적 부족 및 도메인 차이에 의한 과적합과 일반화 능력 저하로 분석된다. 특히, 파인튜닝 데이터의 익명화 및 용어 제한이 모델의 학습을 저해했을 것으로 추측된다. 추가적으로, KoELECTRA 모델과의 비교를 통해 두 모델이 유사한 수준의 높은 분류 정확도(0.90)를 보임을 확인했다. 결론적으로, 폭력성 분류 모델의 안정적인 성능 확보를 위해서는 충분한 양의 정제된 파인튜닝 데이터 확보와 적절한 하이퍼파라미터 튜닝의 필요성을 확인했습니다.

## 개요
방송통신위원회와 한국지능정보사회진흥원의 '2024년도 사이버폭력 실태조사 결과'에 따르면 우리나라 청소년의 42.7%, 성인 13.5%가 사이버폭력을 경험했다고 응답했다. 사이버폭력 경험은 가해와 피해 경험을 모두 포함한다. 2023년 대비 청소년은 1.9%포인트(p), 성인은 5.5%포인트 증가한 수치다. 방송통신위원회는 성별·장애·종교 등이 다르다는 이유로 특정 개인이나 집단에 편견과 차별을 표현하는 '디지털 혐오'나 불법 영상물이나 몰래카메라 등 '디지털 성범죄'와 같은 부정적 콘텐츠에 노출되는 정도가 증가했기 때문이라고 분석했다. 특히 청소년은 성인에 비해 이유가 없거나, 재미·장난으로도 사이버폭력을 행하고 있어 사이버폭력의 심각성을 인지하지 못하는 것으로 나타났다. 이와 같은 배경으로, 대화 텍스트 내 폭력성 유무를 정확하고 신속하게 판단할 수 있는 자동화된 시스템 개발이 사회적으로 매우 중요해졌다. 이를 해결하기 위해 [기존에 있던 프로젝트](https://github.com/NLP-yd10/CheckViolence)의 성능을 검증해보고 파인튜닝하고 다른 모델과 비교하려 한다.


## 목표
한국어 대화 텍스트에서 문장 단위의 폭력성 유무를 자동으로 분류하는 NLP 모델을 검증하고 LoRA (Low-Rank Adaptation) 기법을 적용하여 모델의 파인튜닝 효율성을 극대화하고 성능을 분석한다.


## 통계
실제 상황에서 흔히 접하는 7가지 주요 범주로 분류체계를 정의하고 이 연구에서는 트레이닝 세트 365,500개와 테스트 세트 44,998개로 분할했다. 트레이닝 세트와 테스트 세트는 문장, 폭력성 유무, 폭력성 유형으로 구성되어 있다. 파인튜닝 데이터는 문장과, 폭력성 유무로만 이루어져있다.
<img width="1379" height="750" alt="image" src="https://github.com/user-attachments/assets/0c8168a3-1cf5-4547-b1c6-48225827490a" />


## 사용 데이터
- 텍스트 윤리검증
  - 출처 : [Ai-Hub 텍스트윤리검증](https://aihub.or.kr/aihubdata/data/view.do?srchOptnCnd=OPTNCND001&currMenu=115&topMenu=100&searchKeyword=%ED%85%8D%EC%8A%A4%ED%8A%B8%EC%9C%A4%EB%A6%AC%EA%B2%80%EC%A6%9D&aihubDataSe=data&dataSetSn=558)
  - 데이터 수 : 약 360,000 개
  - 용도 : 모델 학습용

- LLM 학습용 데이터 내 유해표현 검출 AI모델 학습용 데이터
  - 출처 : [Ai-Hub 외부기관데이터 LLM 학습용 데이터 내 유해표현 검출 AI모델 학습용 데이터](https://aihub.or.kr/aihubdata/data/view.do?pageIndex=1&currMenu=118&topMenu=100&srchOptnCnd=OPTNCND001&searchKeyword=%ED%8F%AD%EB%A0%A5&srchDetailCnd=DETAILCND001&srchOrder=ORDER001&srchPagePer=20&aihubDataSe=extrldata&dataSetSn=71833)
  - 데이터 수 : 약 200,000개
  - 용도 : 모델학습용(fine-tuning)
 
 
## 실험결과
- Train Data Size에 따른 F1 Score 변화
  - 트레이닝 데이터개수를 10,000개, 50,000개, 100,000개, 200,000개, 365,500개로 늘리며 트레이닝 후 테스트를 진행하고 유형별로 분석했을 때 HATE, DISCRIMINATION, VIOLENCE는 더이상 f1-score가 향상되지 않는 것을 확인할 수 있었다. 그 외에 CENSURE, ABUSE, CRIME, SEXUAL은 f1-score가 지속적으로 향상되는 것을 확인할 수 있다. 따라서 HATE, DISCRIMINATION, VIOLENCE는 추가적인 트레이닝이 불필요하고 CENSURE, ABUSE, CRIME, SEXUAL은 1,000,000개 이상의 추가적인 트레이닝이 필요할 것으로 추측된다.
    <img width="1370" height="745" alt="image" src="https://github.com/user-attachments/assets/9ef4e822-4675-427e-9354-f1897c58c314" />

- 파인튜닝 후 결과 비교
  - 전체 데이터 세트를 비교한 결과 파인튜닝 하기 전에는 f1-score가 0.90였다. 파인튜닝 후에는 f1-score가 0.72로 크게 떨어졌다. 자세히 확인했을 때 폭력성이 없는 데이터는 0.53로 정확도가 크게 떨어진 것을 알 수 있고 폭력성이 있는 데이터는 0.85로 비교적 정확도가 높은 것을 알 수 있다. 폭력성 유형 별로 비교해보면 CRIME을 제외하곤 파인튜닝 하기 전이 f1-score가 더 높은 것을 확인할 수 있다.
    <img width="1074" height="715" alt="image" src="https://github.com/user-attachments/assets/bed94992-4142-457e-a737-eccefc00e275" />
    
- 다른 모델과의 비교(koELECTRA)
  - ELECTRA는 Replaced Token Detection, 즉 generator에서 나온 token을 보고 discriminator에서 "real" token인지 "fake" token인지 판별하는 방법으로 학습한다. 이 방법은 모든 input token에 대해 학습할 수 있다는 장점을 가지며, BERT 등과 비교했을 때 더 좋은 성능을 보인다.
  - 테스트 결과 전체 정확도는 0.90로 기존 모델과 성능이 크게 차이나지 않았다. 유형별로 분석한 결과 ABUSE와 CRIME에서만 근소하게 기존 모델이 더 뛰어난 성능을 보였고 나머지 유형에선 비슷한 성능을 보였다. 
   <img width="1112" height="709" alt="image" src="https://github.com/user-attachments/assets/703122dd-ced0-4f62-bb17-d6e0ab6d8e83" />

 
- 프로젝트의 정확한 실험결과는 finalProject 폴더와 koELECTRA 폴더를 보면 확인할 수 있다.

## 고찰
- 파인튜닝 후 성능 하락에 대한 고찰 (F1-Score 0.90 -> 0.72)
  - 과적합으로 추정 : 폭력성이 없는 데이터에 대한 정확도(0.53)가 크게 떨어져, 모델이 폭력성이 없는 정상적인 문장을 폭력성 문장으로 오분류하는 경향이 강해진 것으로 해석된다. 파인튜닝을 할 때 학습된 데이터를 확인해봤을 때 폭력성이 없다고 라벨링 되어있지만 욕설이 포함된 문장이 다수 있었고 이로 인한 정확도 하락으로 추정된다.
  - 데이터 도메인 및 전처리 방식의 차이 : 기존 모델 학습 데이터는 실명, 비속어 등 현실적인 대화 요소를 포함한 전체 문장으로 학습된 반면, 파인튜닝 데이터는 익명화 처리나 일부 반사회적 용어로 제한되어 있어 두 데이터셋 간의 도메인 및 표현 방식에 차이가 발생했다. 이러한 도메인 불일치로 인해 모델이 사전 학습된 지식을 효과적으로 활용하지 못하고, 오히려 성능이 저하되었을 것으로 추측된다.
- 유형별 성능 분석 및 데이터 불균형
  - 지속적 향상 필요 유형 (CENSURE, ABUSE, CRIME, SEXUAL) : 이 유형들은 데이터 개수 증가에 따라 F1-Score가 지속적으로 향상되는 추세를 보였다. 이는 이 유형들이 아직 충분한 양의 학습 데이터를 확보하지 못했거나, 분류 경계가 모호하여 추가적인 데이터 학습을 통해 성능 개선 여지가 크다는 것을 시사한다.
  - 향상 불필요 유형 (HATE, DISCRIMINATION, VIOLENCE) : 이 유형들은 데이터 개수가 20만 개 이상일 때 F1-Score가 정체되거나 하락했다. 이는 해당 유형에 대해서는 모델이 이미 데이터의 패턴을 충분히 학습했거나, 현 데이터셋 내에서 해당 유형의 특징이 매우 명확하여 추가적인 데이터가 성능에 미치는 영향이 미미하다는 것을 의미한다.
- 향후 개선 방안
  - 데이터 전처리 및 확보 : 파인튜닝 데이터의 익명화 수준을 조정하거나, 기존 모델 학습 데이터와 유사한 스타일의 고품질 데이터를 추가로 확보하여 도메인 불일치를 최소화해야 한다.
  - 하이퍼파라미터 재설정 : 파인튜닝 과정에서 과적합을 방지하기 위해 Learning Rate를 더 낮추고 조기 종료(Early Stopping) 기준을 엄격하게 적용하며 Epoch 수를 조절하는 재학습 테스트가 필요하다.
  - KoELECTRA 성능 분석 활용: KoELECTRA 모델이 ABUSE와 CRIME에서 KR-BERT보다 우세한 성능을 보였으므로, 두 모델의 앙상블(Ensemble) 기법을 활용하여 전반적인 분류 안정성을 높이는 방안을 고려해 볼 수 있다.

